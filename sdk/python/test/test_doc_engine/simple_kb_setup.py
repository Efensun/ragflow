#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„çŸ¥è¯†åº“è®¾ç½®è„šæœ¬

ç›´æ¥åœ¨ESå’ŒParadeDBä¸­åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œä¸ä¾èµ–å¤æ‚çš„æ•°æ®åº“æœåŠ¡
"""

import os
import sys
import json
import time
import logging
import uuid
from typing import List, Dict, Any
import numpy as np
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag import settings
from rag.utils.es_conn import ESConnection
from rag.utils.pd_conn import PDConnection

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleKBSetup:
    """ç®€åŒ–çš„çŸ¥è¯†åº“è®¾ç½®å™¨"""
    
    def __init__(self):
        self.es_conn = None
        self.pd_conn = None
        self.embedding_model = None
        self.init_connections()
        self.init_embedding_model()
    
    def init_connections(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            # åˆå§‹åŒ–ESè¿æ¥
            if hasattr(settings, 'ES') and settings.ES.get('hosts'):
                logger.info("åˆå§‹åŒ–Elasticsearchè¿æ¥...")
                self.es_conn = ESConnection()
                logger.info("âœ… Elasticsearchè¿æ¥æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æœªé…ç½®Elasticsearch")
            
            # åˆå§‹åŒ–ParadeDBè¿æ¥
            if hasattr(settings, 'PARADEDB') and settings.PARADEDB.get('host'):
                logger.info("åˆå§‹åŒ–ParadeDBè¿æ¥...")
                self.pd_conn = PDConnection()
                logger.info("âœ… ParadeDBè¿æ¥æˆåŠŸ")
            else:
                logger.warning("âš ï¸ æœªé…ç½®ParadeDB")
                
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def init_embedding_model(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        try:
            logger.info("åˆå§‹åŒ–embeddingæ¨¡å‹...")
            
            # å°è¯•ä½¿ç”¨RAGFlowçš„embeddingæ¨¡å‹
            try:
                from rag.nlp import EmbeddingModel
                # ä½¿ç”¨é»˜è®¤çš„embeddingæ¨¡å‹
                model_name = getattr(settings, 'LLM_FACTORY', {}).get('embedding_model', 'BAAI/bge-large-zh-v1.5')
                self.embedding_model = EmbeddingModel(model_name, "")
                logger.info(f"âœ… ä½¿ç”¨RAGFlow embeddingæ¨¡å‹: {model_name}")
                return
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½RAGFlow embeddingæ¨¡å‹: {e}")
            
            # å¤‡é€‰æ–¹æ¡ˆ1: ä½¿ç”¨sentence-transformers
            try:
                from sentence_transformers import SentenceTransformer
                model_name = 'BAAI/bge-large-zh-v1.5'
                self.embedding_model = SentenceTransformer(model_name)
                logger.info(f"âœ… ä½¿ç”¨sentence-transformersæ¨¡å‹: {model_name}")
                return
            except ImportError:
                logger.warning("sentence-transformersæœªå®‰è£…")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½sentence-transformersæ¨¡å‹: {e}")
            
            # å¤‡é€‰æ–¹æ¡ˆ2: ä½¿ç”¨transformers
            try:
                from transformers import AutoTokenizer, AutoModel
                import torch
                
                model_name = 'BAAI/bge-large-zh-v1.5'
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModel.from_pretrained(model_name)
                self.embedding_model = "transformers"
                logger.info(f"âœ… ä½¿ç”¨transformersæ¨¡å‹: {model_name}")
                return
            except ImportError:
                logger.warning("transformersæœªå®‰è£…")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½transformersæ¨¡å‹: {e}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå‘é‡ä½†ç»™å‡ºè­¦å‘Š
            logger.warning("âš ï¸ æ— æ³•åŠ è½½ä»»ä½•embeddingæ¨¡å‹ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡")
            logger.warning("âš ï¸ å»ºè®®å®‰è£…: pip install sentence-transformers")
            self.embedding_model = None
            
        except Exception as e:
            logger.error(f"âŒ embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.embedding_model = None
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬çš„embeddingå‘é‡"""
        try:
            if self.embedding_model is None:
                # ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡ä½œä¸ºå¤‡é€‰
                logger.debug(f"ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡: {text[:50]}...")
                hash_value = hash(text) % (2**32)
                np.random.seed(hash_value)
                vector = np.random.normal(0, 1, 1024)
                vector = vector / np.linalg.norm(vector)
                return vector.tolist()
            
            # RAGFlow embeddingæ¨¡å‹
            if hasattr(self.embedding_model, 'encode'):
                if hasattr(self.embedding_model, 'embed_documents'):
                    # RAGFlow EmbeddingModel
                    embeddings = self.embedding_model.embed_documents([text])
                    return embeddings[0]
                else:
                    # sentence-transformers
                    embedding = self.embedding_model.encode(text, normalize_embeddings=True)
                    return embedding.tolist()
            
            # transformersæ¨¡å‹
            elif self.embedding_model == "transformers":
                import torch
                inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # ä½¿ç”¨[CLS] tokençš„embedding
                    embedding = outputs.last_hidden_state[:, 0, :].squeeze()
                    # å½’ä¸€åŒ–
                    embedding = embedding / torch.norm(embedding)
                    return embedding.numpy().tolist()
            
            else:
                raise Exception("æœªçŸ¥çš„embeddingæ¨¡å‹ç±»å‹")
                
        except Exception as e:
            logger.warning(f"ç”Ÿæˆembeddingå¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå‘é‡")
            # ä½¿ç”¨æ–‡æœ¬hashä½œä¸ºç§å­ï¼Œç¡®ä¿ç›¸åŒæ–‡æœ¬ç”Ÿæˆç›¸åŒå‘é‡
            hash_value = hash(text) % (2**32)
            np.random.seed(hash_value)
            vector = np.random.normal(0, 1, 1024)
            vector = vector / np.linalg.norm(vector)
            return vector.tolist()
    
    def generate_uuid(self) -> str:
        """ç”ŸæˆUUID"""
        return str(uuid.uuid4())
    
    def create_indexes(self, index_name: str, kb_id: str, vector_size: int = 1024):
        """åœ¨ESå’ŒParadeDBä¸­åˆ›å»ºç´¢å¼•"""
        logger.info(f"åˆ›å»ºç´¢å¼•: {index_name}")
        
        # åˆ›å»ºESç´¢å¼•
        if self.es_conn:
            try:
                logger.info(f"åœ¨ESä¸­åˆ›å»ºç´¢å¼•: {index_name}")
                success = self.es_conn.createIdx(index_name, kb_id, vector_size)
                if success:
                    logger.info(f"âœ… ESç´¢å¼•åˆ›å»ºæˆåŠŸ: {index_name}")
                else:
                    logger.info(f"â„¹ï¸ ESç´¢å¼•å·²å­˜åœ¨: {index_name}")
            except Exception as e:
                logger.error(f"âŒ ESç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
                raise
        
        # åˆ›å»ºParadeDBè¡¨
        if self.pd_conn:
            try:
                logger.info(f"åœ¨ParadeDBä¸­åˆ›å»ºè¡¨: {index_name}")
                success = self.pd_conn.createIdx(index_name, kb_id, vector_size)
                if success:
                    logger.info(f"âœ… ParadeDBè¡¨åˆ›å»ºæˆåŠŸ: {index_name}")
                else:
                    logger.info(f"â„¹ï¸ ParadeDBè¡¨å·²å­˜åœ¨: {index_name}")
            except Exception as e:
                logger.error(f"âŒ ParadeDBè¡¨åˆ›å»ºå¤±è´¥: {e}")
                raise
    
    def generate_sample_documents(self, kb_id: str, count: int = 20) -> List[Dict]:
        """ç”Ÿæˆç¤ºä¾‹æ–‡æ¡£æ•°æ®"""
        logger.info(f"ç”Ÿæˆ {count} ä¸ªç¤ºä¾‹æ–‡æ¡£...")
        
        # ç¤ºä¾‹æ–‡æ¡£å†…å®¹
        sample_contents = [
            {
                "title": "äººå·¥æ™ºèƒ½åŸºç¡€çŸ¥è¯†",
                "content": "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æœºå™¨å’Œç³»ç»Ÿã€‚AIåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªå­é¢†åŸŸã€‚ç°ä»£AIç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œå›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ã€‚",
                "keywords": ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è®¡ç®—æœºè§†è§‰", "è‡ªç„¶è¯­è¨€å¤„ç†"]
            },
            {
                "title": "æœºå™¨å­¦ä¹ ç®—æ³•è¯¦è§£",
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚ä¸»è¦åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚ç›‘ç£å­¦ä¹ åŒ…æ‹¬åˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œå¸¸ç”¨ç®—æ³•æœ‰å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚æ— ç›‘ç£å­¦ä¹ ä¸»è¦ç”¨äºèšç±»å’Œé™ç»´ã€‚",
                "keywords": ["æœºå™¨å­¦ä¹ ", "ç›‘ç£å­¦ä¹ ", "æ— ç›‘ç£å­¦ä¹ ", "å¼ºåŒ–å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "å†³ç­–æ ‘"]
            },
            {
                "title": "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯åº”ç”¨",
                "content": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯AIé¢†åŸŸçš„é‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLPæŠ€æœ¯å¹¿æ³›åº”ç”¨äºæœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿã€èŠå¤©æœºå™¨äººç­‰åœºæ™¯ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºTransformeræ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹å¦‚GPTã€BERTç­‰å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚",
                "keywords": ["è‡ªç„¶è¯­è¨€å¤„ç†", "NLP", "æœºå™¨ç¿»è¯‘", "æƒ…æ„Ÿåˆ†æ", "Transformer", "GPT", "BERT"]
            },
            {
                "title": "æ•°æ®ç§‘å­¦ä¸å¤§æ•°æ®åˆ†æ",
                "content": "æ•°æ®ç§‘å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘é¢†åŸŸï¼Œç»“åˆç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†æ¥ä»æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚å¤§æ•°æ®åˆ†ææ¶‰åŠå¤„ç†å’Œåˆ†æå¤§è§„æ¨¡ã€é«˜é€Ÿåº¦ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚å¸¸ç”¨å·¥å…·åŒ…æ‹¬Pythonã€Rã€SQLã€Hadoopã€Sparkç­‰ã€‚æ•°æ®å¯è§†åŒ–æ˜¯æ•°æ®ç§‘å­¦çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚",
                "keywords": ["æ•°æ®ç§‘å­¦", "å¤§æ•°æ®", "æ•°æ®åˆ†æ", "Python", "R", "Hadoop", "Spark", "æ•°æ®å¯è§†åŒ–"]
            },
            {
                "title": "äº‘è®¡ç®—æœåŠ¡å¹³å°",
                "content": "äº‘è®¡ç®—æ˜¯é€šè¿‡äº’è”ç½‘æä¾›å¯æ‰©å±•çš„è®¡ç®—èµ„æºå’ŒæœåŠ¡çš„æ¨¡å¼ã€‚ä¸»è¦æœåŠ¡æ¨¡å‹åŒ…æ‹¬åŸºç¡€è®¾æ–½å³æœåŠ¡(IaaS)ã€å¹³å°å³æœåŠ¡(PaaS)å’Œè½¯ä»¶å³æœåŠ¡(SaaS)ã€‚ä¸»è¦äº‘æœåŠ¡æä¾›å•†åŒ…æ‹¬Amazon AWSã€Microsoft Azureã€Google Cloud Platformç­‰ã€‚äº‘è®¡ç®—å…·æœ‰å¼¹æ€§æ‰©å±•ã€æŒ‰éœ€ä»˜è´¹ã€é«˜å¯ç”¨æ€§ç­‰ä¼˜åŠ¿ã€‚",
                "keywords": ["äº‘è®¡ç®—", "AWS", "Azure", "Google Cloud", "IaaS", "PaaS", "SaaS", "å¼¹æ€§æ‰©å±•"]
            },
            {
                "title": "åŒºå—é“¾æŠ€æœ¯åŸç†ä¸åº”ç”¨",
                "content": "åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œé€šè¿‡å¯†ç å­¦æ–¹æ³•å°†äº¤æ˜“è®°å½•é“¾æ¥æˆä¸å¯ç¯¡æ”¹çš„åŒºå—é“¾ã€‚æ¯ä¸ªåŒºå—åŒ…å«äº¤æ˜“æ•°æ®ã€æ—¶é—´æˆ³å’Œå‰ä¸€ä¸ªåŒºå—çš„å“ˆå¸Œå€¼ã€‚åŒºå—é“¾å…·æœ‰å»ä¸­å¿ƒåŒ–ã€é€æ˜æ€§ã€ä¸å¯ç¯¡æ”¹ç­‰ç‰¹ç‚¹ï¼Œå¹¿æ³›åº”ç”¨äºåŠ å¯†è´§å¸ã€ä¾›åº”é“¾ç®¡ç†ã€æ•°å­—èº«ä»½è®¤è¯ç­‰é¢†åŸŸã€‚",
                "keywords": ["åŒºå—é“¾", "åˆ†å¸ƒå¼è´¦æœ¬", "å¯†ç å­¦", "å»ä¸­å¿ƒåŒ–", "åŠ å¯†è´§å¸", "æ™ºèƒ½åˆçº¦"]
            },
            {
                "title": "Zendeskå®¢æˆ·æœåŠ¡è§£å†³æ–¹æ¡ˆ",
                "content": "Zendeskæ˜¯å…¨çƒé¢†å…ˆçš„å®¢æˆ·æœåŠ¡å’Œæ”¯æŒå¹³å°ï¼Œä¸ºä¼ä¸šæä¾›å…¨æ–¹ä½çš„å®¢æˆ·æœåŠ¡è§£å†³æ–¹æ¡ˆã€‚ä¸»è¦åŠŸèƒ½åŒ…æ‹¬å·¥å•ç®¡ç†ç³»ç»Ÿã€çŸ¥è¯†åº“ã€å®æ—¶èŠå¤©ã€å®¢æˆ·æ»¡æ„åº¦è°ƒæŸ¥ã€å¤šæ¸ é“æ”¯æŒç­‰ã€‚Zendeskå¸®åŠ©ä¼ä¸šæå‡å®¢æˆ·ä½“éªŒï¼Œæé«˜æœåŠ¡æ•ˆç‡ï¼Œæ”¯æŒé‚®ä»¶ã€ç”µè¯ã€ç¤¾äº¤åª’ä½“ã€ç½‘é¡µç­‰å¤šç§æ²Ÿé€šæ¸ é“ã€‚",
                "keywords": ["Zendesk", "å®¢æˆ·æœåŠ¡", "å·¥å•ç³»ç»Ÿ", "çŸ¥è¯†åº“", "å®æ—¶èŠå¤©", "å®¢æˆ·ä½“éªŒ", "å¤šæ¸ é“æ”¯æŒ"]
            },
            {
                "title": "RESTful APIè®¾è®¡è§„èŒƒ",
                "content": "RESTï¼ˆRepresentational State Transferï¼‰æ˜¯ä¸€ç§è½¯ä»¶æ¶æ„é£æ ¼ï¼Œç”¨äºè®¾è®¡ç½‘ç»œåº”ç”¨ç¨‹åºçš„APIã€‚RESTful APIéµå¾ªç»Ÿä¸€æ¥å£ã€æ— çŠ¶æ€ã€å¯ç¼“å­˜ã€åˆ†å±‚ç³»ç»Ÿç­‰åŸåˆ™ã€‚è®¾è®¡æ—¶åº”ä½¿ç”¨æ ‡å‡†HTTPæ–¹æ³•ï¼ˆGETã€POSTã€PUTã€DELETEï¼‰ï¼Œåˆç†è®¾è®¡URLç»“æ„ï¼Œä½¿ç”¨é€‚å½“çš„HTTPçŠ¶æ€ç ï¼Œæä¾›æ¸…æ™°çš„APIæ–‡æ¡£ã€‚",
                "keywords": ["API", "RESTful", "HTTP", "æ¥å£è®¾è®¡", "çŠ¶æ€ç ", "URLè®¾è®¡", "APIæ–‡æ¡£"]
            },
            {
                "title": "æ•°æ®åº“ç®¡ç†ä¸ä¼˜åŒ–",
                "content": "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿï¼ˆDBMSï¼‰æ˜¯ç®¡ç†å’Œæ“ä½œæ•°æ®åº“çš„æ ¸å¿ƒè½¯ä»¶ã€‚å…³ç³»å‹æ•°æ®åº“å¦‚MySQLã€PostgreSQLä½¿ç”¨SQLè¯­è¨€è¿›è¡Œæ•°æ®æ“ä½œï¼Œé€‚åˆç»“æ„åŒ–æ•°æ®å­˜å‚¨ã€‚NoSQLæ•°æ®åº“å¦‚MongoDBã€Redisé€‚åˆå¤„ç†éç»“æ„åŒ–æ•°æ®å’Œé«˜å¹¶å‘åœºæ™¯ã€‚æ•°æ®åº“ä¼˜åŒ–åŒ…æ‹¬ç´¢å¼•è®¾è®¡ã€æŸ¥è¯¢ä¼˜åŒ–ã€åˆ†åº“åˆ†è¡¨ç­‰ç­–ç•¥ã€‚",
                "keywords": ["æ•°æ®åº“", "DBMS", "MySQL", "PostgreSQL", "NoSQL", "MongoDB", "Redis", "ç´¢å¼•ä¼˜åŒ–"]
            },
            {
                "title": "è½¯ä»¶å¼€å‘æœ€ä½³å®è·µ",
                "content": "ç°ä»£è½¯ä»¶å¼€å‘éµå¾ªæ•æ·å¼€å‘æ–¹æ³•è®ºï¼Œå¼ºè°ƒè¿­ä»£å¼€å‘ã€æŒç»­é›†æˆã€è‡ªåŠ¨åŒ–æµ‹è¯•ã€‚DevOpså®è·µå°†å¼€å‘å’Œè¿ç»´ç´§å¯†ç»“åˆï¼Œé€šè¿‡CI/CDæµæ°´çº¿å®ç°å¿«é€Ÿäº¤ä»˜ã€‚ä»£ç è´¨é‡ç®¡ç†åŒ…æ‹¬ä»£ç å®¡æŸ¥ã€å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ç­‰ã€‚ç‰ˆæœ¬æ§åˆ¶ä½¿ç”¨Gitï¼Œé¡¹ç›®ç®¡ç†å¸¸ç”¨Jiraã€Trelloç­‰å·¥å…·ã€‚",
                "keywords": ["è½¯ä»¶å¼€å‘", "æ•æ·å¼€å‘", "DevOps", "CI/CD", "Git", "ä»£ç å®¡æŸ¥", "è‡ªåŠ¨åŒ–æµ‹è¯•"]
            }
        ]
        
        documents = []
        for i in range(count):
            content_data = sample_contents[i % len(sample_contents)]
            
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = self.generate_uuid()
            chunk_id = self.generate_uuid()
            
            # ç”ŸæˆçœŸå®çš„embeddingå‘é‡
            full_text = f"{content_data['title']} {content_data['content']}"
            logger.info(f"ç”Ÿæˆembeddingå‘é‡ {i+1}/{count}: {content_data['title']}")
            vector = self.generate_embedding(full_text)
            
            doc = {
                "id": chunk_id,
                "kb_id": kb_id,
                "doc_id": doc_id,
                "docnm_kwd": f"{content_data['title']}.txt",
                "title_tks": content_data["title"],
                "title_sm_tks": content_data["title"],
                "content_ltks": content_data["content"],
                "content_sm_ltks": content_data["content"],
                "content_with_weight": content_data["content"],
                "important_kwd": content_data["keywords"],
                "important_tks": " ".join(content_data["keywords"]),
                "question_tks": f"ä»€ä¹ˆæ˜¯{content_data['title']}ï¼Ÿå¦‚ä½•ç†è§£{content_data['keywords'][0]}ï¼Ÿ",
                "available_int": 1,
                "create_timestamp_flt": time.time() + i,  # ç¡®ä¿æ—¶é—´æˆ³ä¸åŒ
                "create_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "weight_int": random.randint(1, 10),
                "weight_flt": round(random.uniform(0.1, 1.0), 3),
                "rank_int": random.randint(1, 100),
                "rank_flt": round(random.uniform(0.1, 1.0), 3),
                "page_num_int": [1],
                "top_int": [0],
                "position_int": [0],
                f"q_1024_vec": vector,
                "metadata": {
                    "source": "test_data",
                    "type": "text",
                    "language": "zh",
                    "index": i,
                    "embedding_model": str(type(self.embedding_model).__name__) if self.embedding_model else "simulated"
                }
            }
            documents.append(doc)
        
        logger.info(f"âœ… ç”Ÿæˆäº† {len(documents)} ä¸ªç¤ºä¾‹æ–‡æ¡£ï¼ˆä½¿ç”¨çœŸå®embeddingå‘é‡ï¼‰")
        return documents
    
    def insert_documents(self, documents: List[Dict], index_name: str, kb_id: str):
        """å°†æ–‡æ¡£æ’å…¥åˆ°ESå’ŒParadeDB"""
        logger.info(f"å¼€å§‹æ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£...")
        
        es_success = False
        pd_success = False
        
        # æ’å…¥åˆ°ES
        if self.es_conn:
            try:
                logger.info("æ’å…¥æ–‡æ¡£åˆ°Elasticsearch...")
                errors = self.es_conn.insert(documents, index_name, kb_id)
                if errors:
                    logger.warning(f"ESæ’å…¥æ—¶å‡ºç° {len(errors)} ä¸ªé”™è¯¯: {errors[:3]}...")
                else:
                    logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£åˆ°ES")
                    es_success = True
            except Exception as e:
                logger.error(f"âŒ ESæ–‡æ¡£æ’å…¥å¤±è´¥: {e}")
        
        # æ’å…¥åˆ°ParadeDB
        if self.pd_conn:
            try:
                logger.info("æ’å…¥æ–‡æ¡£åˆ°ParadeDB...")
                errors = self.pd_conn.insert(documents, index_name, kb_id)
                if errors:
                    logger.warning(f"ParadeDBæ’å…¥æ—¶å‡ºç° {len(errors)} ä¸ªé”™è¯¯: {errors[:3]}...")
                else:
                    logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£åˆ°ParadeDB")
                    pd_success = True
            except Exception as e:
                logger.error(f"âŒ ParadeDBæ–‡æ¡£æ’å…¥å¤±è´¥: {e}")
        
        return es_success, pd_success
    
    def verify_data(self, index_name: str, kb_id: str) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        logger.info("éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
        
        result = {
            "es_count": 0,
            "pd_count": 0,
            "consistent": False,
            "es_sample": None,
            "pd_sample": None
        }
        
        # æ£€æŸ¥ES
        if self.es_conn:
            try:
                from rag.utils.doc_store_conn import OrderByExpr
                es_res = self.es_conn.search(
                    selectFields=["id", "title_tks", "content_with_weight"],
                    highlightFields=[],
                    condition={"available_int": 1},
                    matchExprs=[],
                    orderBy=OrderByExpr(),
                    offset=0,
                    limit=100,
                    indexNames=index_name,
                    knowledgebaseIds=[kb_id]
                )
                result["es_count"] = self.es_conn.getTotal(es_res)
                
                # è·å–æ ·æœ¬
                hits = es_res.get("hits", {}).get("hits", [])
                if hits:
                    sample = hits[0]
                    result["es_sample"] = {
                        "id": sample.get("_id"),
                        "title": sample.get("_source", {}).get("title_tks", ""),
                        "content_preview": sample.get("_source", {}).get("content_with_weight", "")[:100]
                    }
                
            except Exception as e:
                logger.error(f"ESæ•°æ®éªŒè¯å¤±è´¥: {e}")
        
        # æ£€æŸ¥ParadeDB
        if self.pd_conn:
            try:
                from rag.utils.doc_store_conn import OrderByExpr
                pd_res = self.pd_conn.search(
                    selectFields=["id", "title_tks", "content_with_weight"],
                    highlightFields=[],
                    condition={"available_int": 1},
                    matchExprs=[],
                    orderBy=OrderByExpr(),
                    offset=0,
                    limit=100,
                    indexNames=index_name,
                    knowledgebaseIds=[kb_id]
                )
                result["pd_count"] = self.pd_conn.getTotal(pd_res)
                
                # è·å–æ ·æœ¬
                hits = pd_res.get("hits", {}).get("hits", [])
                if hits:
                    sample = hits[0]
                    result["pd_sample"] = {
                        "id": sample.get("_id"),
                        "title": sample.get("_source", {}).get("title_tks", ""),
                        "content_preview": sample.get("_source", {}).get("content_with_weight", "")[:100]
                    }
                
            except Exception as e:
                logger.error(f"ParadeDBæ•°æ®éªŒè¯å¤±è´¥: {e}")
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        result["consistent"] = (
            result["es_count"] == result["pd_count"] and 
            result["es_count"] > 0
        )
        
        return result
    
    def setup_test_knowledge_base(self, kb_name: str = "æµ‹è¯•çŸ¥è¯†åº“", 
                                 document_count: int = 20) -> Dict[str, Any]:
        """è®¾ç½®æµ‹è¯•çŸ¥è¯†åº“"""
        logger.info(f"å¼€å§‹è®¾ç½®æµ‹è¯•çŸ¥è¯†åº“: {kb_name}")
        
        # ç”ŸæˆID
        kb_id = self.generate_uuid()
        tenant_id = self.generate_uuid()
        index_name = f"ragflow_{tenant_id.replace('-', '')}"
        
        try:
            # 1. åˆ›å»ºç´¢å¼•
            self.create_indexes(index_name, kb_id)
            
            # 2. ç”Ÿæˆæ–‡æ¡£
            documents = self.generate_sample_documents(kb_id, document_count)
            
            # 3. æ’å…¥æ–‡æ¡£
            es_success, pd_success = self.insert_documents(documents, index_name, kb_id)
            
            # 4. ç­‰å¾…ç´¢å¼•åˆ·æ–°
            logger.info("ç­‰å¾…ç´¢å¼•åˆ·æ–°...")
            time.sleep(5)
            
            # 5. éªŒè¯æ•°æ®
            verification = self.verify_data(index_name, kb_id)
            
            result = {
                "kb_id": kb_id,
                "tenant_id": tenant_id,
                "index_name": index_name,
                "kb_name": kb_name,
                "document_count": document_count,
                "es_success": es_success,
                "pd_success": pd_success,
                "verification": verification,
                "success": verification["consistent"]
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“è®¾ç½®å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–çŸ¥è¯†åº“è®¾ç½®å·¥å…·ï¼ˆä½¿ç”¨çœŸå®embeddingå‘é‡ï¼‰")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–è®¾ç½®å™¨
        setup = SimpleKBSetup()
        
        if not setup.es_conn and not setup.pd_conn:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æœç´¢å¼•æ“è¿æ¥")
            return
        
        # è®¾ç½®çŸ¥è¯†åº“
        result = setup.setup_test_knowledge_base(
            kb_name="ESä¸ParadeDBå¯¹æ¯”æµ‹è¯•çŸ¥è¯†åº“ï¼ˆçœŸå®å‘é‡ï¼‰",
            document_count=20
        )
        
        print(f"\nğŸ“Š è®¾ç½®ç»“æœ:")
        print(f"  çŸ¥è¯†åº“åç§°: {result['kb_name']}")
        print(f"  çŸ¥è¯†åº“ID: {result['kb_id']}")
        print(f"  ç´¢å¼•åç§°: {result['index_name']}")
        print(f"  ç§Ÿæˆ·ID: {result['tenant_id']}")
        print(f"  æ–‡æ¡£æ•°é‡: {result['document_count']}")
        print(f"  Embeddingæ¨¡å‹: {str(type(setup.embedding_model).__name__) if setup.embedding_model else 'simulated'}")
        print(f"  ESæ’å…¥æˆåŠŸ: {'âœ…' if result['es_success'] else 'âŒ'}")
        print(f"  ParadeDBæ’å…¥æˆåŠŸ: {'âœ…' if result['pd_success'] else 'âŒ'}")
        
        verification = result['verification']
        print(f"\nğŸ” æ•°æ®éªŒè¯:")
        print(f"  ESæ–‡æ¡£æ•°: {verification['es_count']}")
        print(f"  ParadeDBæ–‡æ¡£æ•°: {verification['pd_count']}")
        print(f"  æ•°æ®ä¸€è‡´æ€§: {'âœ…' if verification['consistent'] else 'âŒ'}")
        
        if verification['es_sample']:
            print(f"\nğŸ“„ ESæ ·æœ¬æ–‡æ¡£:")
            print(f"  ID: {verification['es_sample']['id']}")
            print(f"  æ ‡é¢˜: {verification['es_sample']['title']}")
            print(f"  å†…å®¹é¢„è§ˆ: {verification['es_sample']['content_preview']}...")
        
        if verification['pd_sample']:
            print(f"\nğŸ“„ ParadeDBæ ·æœ¬æ–‡æ¡£:")
            print(f"  ID: {verification['pd_sample']['id']}")
            print(f"  æ ‡é¢˜: {verification['pd_sample']['title']}")
            print(f"  å†…å®¹é¢„è§ˆ: {verification['pd_sample']['content_preview']}...")
        
        if result['success']:
            print(f"\nğŸ‰ çŸ¥è¯†åº“è®¾ç½®æˆåŠŸ!")
            
            # ç”Ÿæˆæµ‹è¯•é…ç½®
            config_content = f'''# è‡ªåŠ¨ç”Ÿæˆçš„æœç´¢å¯¹æ¯”æµ‹è¯•é…ç½®ï¼ˆä½¿ç”¨çœŸå®embeddingå‘é‡ï¼‰
# ç”Ÿæˆæ—¶é—´: {time.strftime("%Y-%m-%d %H:%M:%S")}
# Embeddingæ¨¡å‹: {str(type(setup.embedding_model).__name__) if setup.embedding_model else 'simulated'}

INDEX_NAME = "{result['index_name']}"
KB_ID = "{result['kb_id']}"
TENANT_ID = "{result['tenant_id']}"

# ç”¨äº quick_search_test.py
TEST_CONFIG = {{
    "index_name": "{result['index_name']}",
    "kb_ids": ["{result['kb_id']}"],
    "test_queries": [
        "äººå·¥æ™ºèƒ½åŸºç¡€çŸ¥è¯†",
        "æœºå™¨å­¦ä¹ ç®—æ³•", 
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
        "æ•°æ®ç§‘å­¦åˆ†æ",
        "äº‘è®¡ç®—å¹³å°",
        "åŒºå—é“¾æŠ€æœ¯",
        "Zendeskå®¢æˆ·æœåŠ¡",
        "APIæ¥å£è®¾è®¡",
        "æ•°æ®åº“ç®¡ç†",
        "è½¯ä»¶å¼€å‘å®è·µ",
        # è¯­ä¹‰ç›¸å…³çš„æŸ¥è¯¢
        "AIå’Œæ·±åº¦å­¦ä¹ ",
        "NLPå’Œæ–‡æœ¬åˆ†æ",
        "å¤§æ•°æ®å¤„ç†",
        "åˆ†å¸ƒå¼ç³»ç»Ÿ",
        "å®¢æˆ·æ”¯æŒå¹³å°"
    ]
}}

# ç”¨äº compare_search_engines.py
COMPARISON_CONFIG = {{
    "index_name": "{result['index_name']}",
    "kb_ids": ["{result['kb_id']}"],
    "vector_weights": [0.2, 0.3, 0.5, 0.7, 0.8],  # æ›´å¤šæƒé‡æµ‹è¯•
    "test_queries": TEST_CONFIG["test_queries"]
}}
'''
            
            with open("test_kb_config.py", "w", encoding="utf-8") as f:
                f.write(config_content)
            
            print(f"\nğŸ“„ å·²ç”Ÿæˆæµ‹è¯•é…ç½®æ–‡ä»¶: test_kb_config.py")
            print(f"\nğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œæœç´¢å¯¹æ¯”æµ‹è¯•:")
            print(f"  python quick_search_test.py")
            print(f"  python compare_search_engines.py")
            print(f"\nğŸ¯ ä½¿ç”¨çœŸå®embeddingå‘é‡ï¼Œå¯ä»¥éªŒè¯è¯­ä¹‰ç›¸å…³æ€§ï¼")
            
        else:
            print(f"\nâŒ çŸ¥è¯†åº“è®¾ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            
    except Exception as e:
        logger.error(f"è®¾ç½®è¿‡ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæœç´¢å¼•æ“å¯¹æ¯”æµ‹è¯•è„šæœ¬ï¼ˆæ”¯æŒçœŸå®embeddingå‘é‡ï¼‰

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œç”¨äºå¿«é€ŸéªŒè¯ESå’ŒParadeDBçš„åŸºæœ¬æœç´¢åŠŸèƒ½ï¼Œä½¿ç”¨çœŸå®embeddingè¿›è¡Œè¯­ä¹‰æœç´¢æµ‹è¯•
"""

import os
import sys
import time
import logging
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag import settings
from rag.utils.es_conn import ESConnection
from rag.utils.pd_conn import PDConnection
from rag.utils.doc_store_conn import MatchTextExpr, MatchDenseExpr, FusionExpr, OrderByExpr
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QuickSearchTester:
    """å¿«é€Ÿæœç´¢æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.es_conn = None
        self.pd_conn = None
        self.embedding_model = None
        self.init_connections()
        self.init_embedding_model()
    
    def init_connections(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            # åˆå§‹åŒ–ESè¿æ¥
            if hasattr(settings, 'ES') and settings.ES.get('hosts'):
                print("ğŸ”— è¿æ¥Elasticsearch...")
                self.es_conn = ESConnection()
                print("âœ… ESè¿æ¥æˆåŠŸ")
            else:
                print("âš ï¸ æœªé…ç½®ESï¼Œè·³è¿‡ESæµ‹è¯•")
        except Exception as e:
            print(f"âŒ ESè¿æ¥å¤±è´¥: {e}")
        
        try:
            # åˆå§‹åŒ–ParadeDBè¿æ¥
            if hasattr(settings, 'PARADEDB') and settings.PARADEDB.get('host'):
                print("ğŸ”— è¿æ¥ParadeDB...")
                self.pd_conn = PDConnection()
                print("âœ… ParadeDBè¿æ¥æˆåŠŸ")
            else:
                print("âš ï¸ æœªé…ç½®ParadeDBï¼Œè·³è¿‡ParadeDBæµ‹è¯•")
        except Exception as e:
            print(f"âŒ ParadeDBè¿æ¥å¤±è´¥: {e}")
    
    def init_embedding_model(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆä¸simple_kb_setup.pyä¿æŒä¸€è‡´ï¼‰"""
        try:
            # å°è¯•ä½¿ç”¨RAGFlowçš„embeddingæ¨¡å‹
            try:
                from rag.nlp import EmbeddingModel
                model_name = getattr(settings, 'LLM_FACTORY', {}).get('embedding_model', 'BAAI/bge-large-zh-v1.5')
                self.embedding_model = EmbeddingModel(model_name, "")
                print(f"âœ… ä½¿ç”¨RAGFlow embeddingæ¨¡å‹: {model_name}")
                return
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½RAGFlow embeddingæ¨¡å‹: {e}")
            
            # å¤‡é€‰æ–¹æ¡ˆ1: ä½¿ç”¨sentence-transformers
            try:
                from sentence_transformers import SentenceTransformer
                model_name = 'BAAI/bge-large-zh-v1.5'
                self.embedding_model = SentenceTransformer(model_name)
                print(f"âœ… ä½¿ç”¨sentence-transformersæ¨¡å‹: {model_name}")
                return
            except ImportError:
                logger.warning("sentence-transformersæœªå®‰è£…")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½sentence-transformersæ¨¡å‹: {e}")
            
            # å¤‡é€‰æ–¹æ¡ˆ2: ä½¿ç”¨transformers
            try:
                from transformers import AutoTokenizer, AutoModel
                import torch
                
                model_name = 'BAAI/bge-large-zh-v1.5'
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModel.from_pretrained(model_name)
                self.embedding_model = "transformers"
                print(f"âœ… ä½¿ç”¨transformersæ¨¡å‹: {model_name}")
                return
            except ImportError:
                logger.warning("transformersæœªå®‰è£…")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½transformersæ¨¡å‹: {e}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå‘é‡
            print("âš ï¸ æ— æ³•åŠ è½½ä»»ä½•embeddingæ¨¡å‹ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡")
            self.embedding_model = None
            
        except Exception as e:
            logger.error(f"âŒ embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.embedding_model = None
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬çš„embeddingå‘é‡ï¼ˆä¸simple_kb_setup.pyä¿æŒä¸€è‡´ï¼‰"""
        try:
            if self.embedding_model is None:
                # ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡ä½œä¸ºå¤‡é€‰
                hash_value = hash(text) % (2**32)
                np.random.seed(hash_value)
                vector = np.random.normal(0, 1, 1024)
                vector = vector / np.linalg.norm(vector)
                return vector.tolist()
            
            # RAGFlow embeddingæ¨¡å‹
            if hasattr(self.embedding_model, 'encode'):
                if hasattr(self.embedding_model, 'embed_documents'):
                    # RAGFlow EmbeddingModel
                    embeddings = self.embedding_model.embed_documents([text])
                    return embeddings[0]
                else:
                    # sentence-transformers
                    embedding = self.embedding_model.encode(text, normalize_embeddings=True)
                    return embedding.tolist()
            
            # transformersæ¨¡å‹
            elif self.embedding_model == "transformers":
                import torch
                inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # ä½¿ç”¨[CLS] tokençš„embedding
                    embedding = outputs.last_hidden_state[:, 0, :].squeeze()
                    # å½’ä¸€åŒ–
                    embedding = embedding / torch.norm(embedding)
                    return embedding.numpy().tolist()
            
            else:
                raise Exception("æœªçŸ¥çš„embeddingæ¨¡å‹ç±»å‹")
                
        except Exception as e:
            logger.warning(f"ç”Ÿæˆembeddingå¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå‘é‡")
            # ä½¿ç”¨æ–‡æœ¬hashä½œä¸ºç§å­ï¼Œç¡®ä¿ç›¸åŒæ–‡æœ¬ç”Ÿæˆç›¸åŒå‘é‡
            hash_value = hash(text) % (2**32)
            np.random.seed(hash_value)
            vector = np.random.normal(0, 1, 1024)
            vector = vector / np.linalg.norm(vector)
            return vector.tolist()
    
    def create_simple_search(self, query_text):
        """åˆ›å»ºç®€å•çš„æ–‡æœ¬æœç´¢è¡¨è¾¾å¼"""
        return [MatchTextExpr(
            fields=["content_ltks^10", "title_tks^8", "content_with_weight^2"],
            matching_text=query_text,
            topn=10,
            extra_options={"minimum_should_match": 0.3}
        )]
    
    def create_vector_search(self, query_text):
        """åˆ›å»ºå‘é‡æœç´¢è¡¨è¾¾å¼ï¼ˆä½¿ç”¨çœŸå®embeddingï¼‰"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.generate_embedding(query_text)
        print(f"  æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vector)}")
        
        return [MatchDenseExpr(
            vector_column_name="q_1024_vec",
            embedding_data=query_vector,
            topn=10,
            extra_options={"similarity": 0.1}
        )]
    
    def create_hybrid_search(self, query_text, vector_weight=0.5):
        """åˆ›å»ºæ··åˆæœç´¢è¡¨è¾¾å¼ï¼ˆæ–‡æœ¬+çœŸå®å‘é‡ï¼‰"""
        expressions = []
        
        # æ–‡æœ¬æœç´¢
        text_expr = MatchTextExpr(
            fields=["content_ltks^10", "title_tks^8", "content_with_weight^2"],
            matching_text=query_text,
            topn=10,
            extra_options={"minimum_should_match": 0.3}
        )
        expressions.append(text_expr)
        
        # å‘é‡æœç´¢ï¼ˆä½¿ç”¨çœŸå®embeddingï¼‰
        query_vector = self.generate_embedding(query_text)
        vector_expr = MatchDenseExpr(
            vector_column_name="q_1024_vec",
            embedding_data=query_vector,
            topn=10,
            extra_options={"similarity": 0.1}
        )
        expressions.append(vector_expr)
        
        # èåˆè¡¨è¾¾å¼
        text_weight = 1.0 - vector_weight
        fusion_expr = FusionExpr(
            method="weighted_sum",
            topn=10,
            fusion_params={"weights": f"{text_weight}, {vector_weight}"}
        )
        expressions.append(fusion_expr)
        
        return expressions

def test_basic_search():
    """æµ‹è¯•åŸºæœ¬æœç´¢åŠŸèƒ½"""
    print("ğŸ” å¿«é€Ÿæœç´¢å¼•æ“å¯¹æ¯”æµ‹è¯•ï¼ˆä½¿ç”¨çœŸå®embeddingå‘é‡ï¼‰")
    print("=" * 60)
    
    # å°è¯•åŠ è½½è‡ªåŠ¨ç”Ÿæˆçš„é…ç½®
    try:
        from test_kb_config import TEST_CONFIG
        INDEX_NAME = TEST_CONFIG["index_name"]
        KB_IDS = TEST_CONFIG["kb_ids"]
        TEST_QUERIES = TEST_CONFIG["test_queries"][:3]  # ä½¿ç”¨å‰3ä¸ªæµ‹è¯•æŸ¥è¯¢
        print("âœ… ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„é…ç½®")
    except ImportError:
        # å¦‚æœæ²¡æœ‰é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼
        print("âš ï¸ æœªæ‰¾åˆ°test_kb_config.pyï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        print("ğŸ’¡ å»ºè®®å…ˆè¿è¡Œ python simple_kb_setup.py åˆ›å»ºæµ‹è¯•æ•°æ®")
        INDEX_NAME = "ragflow_6a2a5a8c00a611f0883a0242ac140006"
        KB_IDS = ["6a2a5a8c-00a6-11f0-883a-0242ac140006"]
        TEST_QUERIES = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†"]
    
    print(f"ç´¢å¼•åç§°: {INDEX_NAME}")
    print(f"çŸ¥è¯†åº“ID: {KB_IDS}")
    print(f"æµ‹è¯•æŸ¥è¯¢: {TEST_QUERIES}")
    print()
    
    # åˆå§‹åŒ–æµ‹è¯•å™¨
    tester = QuickSearchTester()
    
    if not tester.es_conn and not tester.pd_conn:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æœç´¢å¼•æ“è¿æ¥")
        return
    
    print(f"Embeddingæ¨¡å‹: {type(tester.embedding_model).__name__ if tester.embedding_model else 'simulated'}")
    print()
    
    # æµ‹è¯•æ¯ä¸ªæŸ¥è¯¢
    for query_idx, test_query in enumerate(TEST_QUERIES):
        print(f"ğŸ§ª æµ‹è¯•æŸ¥è¯¢ {query_idx + 1}/{len(TEST_QUERIES)}: '{test_query}'")
        print("=" * 50)
        
        # æµ‹è¯•ä¸åŒçš„æœç´¢æ–¹å¼
        test_cases = [
            ("çº¯æ–‡æœ¬æœç´¢", tester.create_simple_search(test_query)),
            ("çº¯å‘é‡æœç´¢", tester.create_vector_search(test_query)),
            ("æ··åˆæœç´¢(0.5)", tester.create_hybrid_search(test_query, 0.5))
        ]
        
        for test_name, match_exprs in test_cases:
            print(f"\nğŸ” {test_name}")
            print("-" * 30)
            
            # ESæœç´¢
            if tester.es_conn:
                try:
                    start_time = time.time()
                    es_res = tester.es_conn.search(
                        selectFields=["id", "title_tks", "content_with_weight"],
                        highlightFields=[],
                        condition={"available_int": 1},
                        matchExprs=match_exprs,
                        orderBy=OrderByExpr(),
                        offset=0,
                        limit=5,
                        indexNames=INDEX_NAME,
                        knowledgebaseIds=KB_IDS
                    )
                    es_time = time.time() - start_time
                    
                    es_hits = es_res.get("hits", {}).get("hits", [])
                    print(f"ESç»“æœ: {len(es_hits)}æ¡, è€—æ—¶{es_time:.3f}ç§’")
                    
                    for i, hit in enumerate(es_hits[:3]):
                        source = hit.get("_source", {})
                        title = source.get("title_tks", "")
                        score = hit.get("_score", 0)
                        print(f"  {i+1}. {title} (åˆ†æ•°: {score:.3f})")
                        
                except Exception as e:
                    print(f"ESæœç´¢å¤±è´¥: {e}")
            
            # ParadeDBæœç´¢
            if tester.pd_conn:
                try:
                    start_time = time.time()
                    pd_res = tester.pd_conn.search(
                        selectFields=["id", "title_tks", "content_with_weight"],
                        highlightFields=[],
                        condition={"available_int": 1},
                        matchExprs=match_exprs,
                        orderBy=OrderByExpr(),
                        offset=0,
                        limit=5,
                        indexNames=INDEX_NAME,
                        knowledgebaseIds=KB_IDS
                    )
                    pd_time = time.time() - start_time
                    
                    pd_hits = pd_res.get("hits", {}).get("hits", [])
                    print(f"ParadeDBç»“æœ: {len(pd_hits)}æ¡, è€—æ—¶{pd_time:.3f}ç§’")
                    
                    for i, hit in enumerate(pd_hits[:3]):
                        source = hit.get("_source", {})
                        title = source.get("title_tks", "")
                        score = hit.get("_score", 0)
                        print(f"  {i+1}. {title} (åˆ†æ•°: {score:.3f})")
                        
                except Exception as e:
                    print(f"ParadeDBæœç´¢å¤±è´¥: {e}")
            
            # ç®€å•å¯¹æ¯”
            if tester.es_conn and tester.pd_conn:
                try:
                    es_hits = es_res.get("hits", {}).get("hits", [])
                    pd_hits = pd_res.get("hits", {}).get("hits", [])
                    
                    if es_hits and pd_hits:
                        # æå–æ–‡æ¡£IDè¿›è¡Œå¯¹æ¯”
                        es_ids = [hit.get("_id") for hit in es_hits]
                        pd_ids = [hit.get("_id") for hit in pd_hits]
                        
                        # è®¡ç®—é‡å 
                        common_ids = set(es_ids) & set(pd_ids)
                        overlap_ratio = len(common_ids) / max(len(es_ids), len(pd_ids)) if max(len(es_ids), len(pd_ids)) > 0 else 0
                        
                        print(f"ğŸ“Š ç»“æœå¯¹æ¯”: é‡å ç‡ {overlap_ratio:.2%} ({len(common_ids)}/{max(len(es_ids), len(pd_ids))})")
                        
                except Exception as e:
                    print(f"ç»“æœå¯¹æ¯”å¤±è´¥: {e}")
        
        print()  # æŸ¥è¯¢é—´çš„åˆ†éš”
    
    print("ğŸ‰ å¿«é€Ÿæµ‹è¯•å®Œæˆ!")
    print("ğŸ’¡ å¦‚éœ€è¯¦ç»†åˆ†æï¼Œè¯·è¿è¡Œ: python compare_search_engines.py")

def main():
    """ä¸»å‡½æ•°"""
    try:
        test_basic_search()
        print("âœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ")
        print("\nğŸ’¡ æç¤º: å¦‚éœ€è¯¦ç»†åˆ†æï¼Œè¯·ä½¿ç”¨ compare_search_engines.py")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
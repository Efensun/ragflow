#
#  Copyright 2025 The InfiniFlow Authors. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#

import logging
import json
import time
import psycopg2
from psycopg2.extras import RealDictCursor
from psycopg2.pool import SimpleConnectionPool
from rag import settings
from rag.utils import singleton
from rag.utils.doc_store_conn import DocStoreConnection, MatchExpr, OrderByExpr, MatchTextExpr, MatchDenseExpr, \
    FusionExpr
import re
import copy
import hashlib
from rag.nlp import rag_tokenizer

ATTEMPT_TIME = 2
logger = logging.getLogger('ragflow.pd_conn')


@singleton
class PDConnection(DocStoreConnection):
    def __init__(self):
        self.info = {}
        logger.info(f"Use ParadeDB {settings.PARADEDB['host']} as the doc engine.")

        # è¿æ¥æ± çŠ¶æ€ç›‘æ§å˜é‡
        self._total_acquired = 0
        self._total_released = 0
        self._last_connection_check = time.time()
        self._connection_check_interval = 60  # æ¯60ç§’æ£€æŸ¥ä¸€æ¬¡è¿æ¥çŠ¶æ€

        for _ in range(ATTEMPT_TIME):
            try:
                # åˆ›å»ºè¿æ¥æ±  - å¢åŠ æœ€å¤§è¿æ¥æ•°
                self.pool = SimpleConnectionPool(
                    minconn=1,
                    maxconn=settings.PARADEDB.get("max_connections", 50),  # é»˜è®¤æé«˜åˆ°50
                    host=settings.PARADEDB["host"],
                    port=settings.PARADEDB["port"],
                    database=settings.PARADEDB.get("database", settings.PARADEDB.get("name", "rag_flow")),
                    user=settings.PARADEDB["user"],
                    password=settings.PARADEDB["password"]
                )

                # æµ‹è¯•è¿æ¥
                conn = self._get_connection()
                try:
                    with conn.cursor() as cur:
                        cur.execute("SELECT version()")
                        self.info["version"] = cur.fetchone()[0]
                finally:
                    # ç¡®ä¿æµ‹è¯•è¿æ¥å½’è¿˜åˆ°æ± ä¸­
                    self._return_connection(conn)
                break
            except Exception as e:
                logger.warning(f"{str(e)}. Waiting ParadeDB {settings.PARADEDB['host']} to be healthy.")
                time.sleep(5)

        if not self.info:
            msg = f"ParadeDB {settings.PARADEDB['host']} is unhealthy in 120s."
            logger.error(msg)
            raise Exception(msg)

        logger.info(f"ParadeDB {settings.PARADEDB['host']} is healthy.")

    def _get_connection(self):
        """è·å–è¿æ¥å¹¶è®°å½•è¿æ¥è®¡æ•°"""
        self._check_connection_pool()
        try:
            conn = self.pool.getconn()
            self._total_acquired += 1
            return ManagedConnection(self, conn)
        except psycopg2.pool.PoolError as e:
            # å¦‚æœæ± å·²è€—å°½ï¼Œå°è¯•é‡ç½®è¿æ¥æ± 
            logger.error(f"Connection pool error: {str(e)}. Trying to reset connections.")
            self._reset_connection_pool()
            return ManagedConnection(self, self.pool.getconn())

    def _return_connection(self, conn):
        """å½’è¿˜è¿æ¥å¹¶è®°å½•è¿æ¥è®¡æ•°"""
        if not conn:
            return
            
        try:
            # å¦‚æœconnæ˜¯ManagedConnectionå®ä¾‹ï¼Œè·å–å®é™…è¿æ¥
            if hasattr(conn, 'conn'):
                actual_conn = conn.conn
                # æ ‡è®°ä¸ºå·²å½’è¿˜ï¼Œé¿å…é‡å¤å½’è¿˜
                if hasattr(conn, '_returned') and conn._returned:
                    return
                if hasattr(conn, '_returned'):
                    conn._returned = True
            else:
                actual_conn = conn
            
            # æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
            if not actual_conn or actual_conn.closed:
                logger.debug("Connection is already closed, skipping return")
                return
            
            # å°è¯•å½’è¿˜è¿æ¥åˆ°æ± ä¸­
            try:
                self.pool.putconn(actual_conn)
                self._total_released += 1
                logger.debug("Successfully returned connection to pool")
            except psycopg2.pool.PoolError as e:
                # è¿æ¥å¯èƒ½ä¸åœ¨æ± ä¸­æˆ–å·²ç»å½’è¿˜
                logger.debug(f"Pool error when returning connection: {str(e)}")
            
        except Exception as e:
            logger.debug(f"Error returning connection to pool: {str(e)}")  # æ”¹ä¸ºdebugçº§åˆ«

    def _check_connection_pool(self):
        """æ£€æŸ¥è¿æ¥æ± çŠ¶æ€ï¼Œå¦‚æœå‘ç°æ³„æ¼åˆ™å°è¯•ä¿®å¤"""
        now = time.time()
        if now - self._last_connection_check > self._connection_check_interval:
            self._last_connection_check = now
            
            # è®¡ç®—æœªé‡Šæ”¾çš„è¿æ¥æ•°
            leaked = self._total_acquired - self._total_released
            
            # è®°å½•è¿æ¥æ± çŠ¶æ€
            logger.info(f"Connection pool status: acquired={self._total_acquired}, released={self._total_released}, leaked={leaked}, used={self.pool._used}")
            
            # å¦‚æœæ³„æ¼è¿æ¥å¤ªå¤šï¼Œå°è¯•é‡ç½®è¿æ¥æ± 
            if leaked > 10:
                logger.warning(f"Detected {leaked} leaked connections. Attempting to reset connection pool.")
                self._reset_connection_pool()

    def _reset_connection_pool(self):
        """å°è¯•å…³é—­æ‰€æœ‰è¿æ¥å¹¶é‡ç½®è¿æ¥æ± """
        try:
            # å…³é—­æ‰€æœ‰è¿æ¥
            self.pool.closeall()
            
            # åˆ›å»ºæ–°çš„è¿æ¥æ± 
            self.pool = SimpleConnectionPool(
                minconn=1,
                maxconn=settings.PARADEDB.get("max_connections", 50),
                host=settings.PARADEDB["host"],
                port=settings.PARADEDB["port"],
                database=settings.PARADEDB.get("database", settings.PARADEDB.get("name", "rag_flow")),
                user=settings.PARADEDB["user"],
                password=settings.PARADEDB["password"]
            )
            
            # é‡ç½®è®¡æ•°å™¨
            self._total_acquired = 0
            self._total_released = 0
            
            logger.info("Connection pool has been reset successfully.")
        except Exception as e:
            logger.error(f"Failed to reset connection pool: {str(e)}")

    """
    Database operations
    """

    def dbType(self) -> str:
        return "paradedb"

    def health(self) -> dict:
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                cur.execute("""
                    SELECT * FROM pg_stat_database 
                    WHERE datname = current_database()
                """)
                health_dict = dict(cur.fetchone())
                health_dict["type"] = "paradedb"
                # æ·»åŠ è¿æ¥æ± çŠ¶æ€ä¿¡æ¯
                health_dict["pool_acquired"] = self._total_acquired
                health_dict["pool_released"] = self._total_released
                health_dict["pool_used"] = len(self.pool._used)
                return health_dict
        except Exception as e:
            logger.exception(f"PDConnection.health got exception: {str(e)}")
            return {"type": "paradedb", "status": "error", "error": str(e)}
        finally:
            if conn:
                self._return_connection(conn)

    """
    Table operations
    """

    def createIdx(self, indexName: str, knowledgebaseId: str, vectorSize: int):
        # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
        if self.indexExist(indexName):
            logger.info(f"Table {indexName} already exists, checking schema...")
            # å‡çº§è¡¨ç»“æ„ä»¥ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
            if self._upgrade_table_schema(indexName):
                logger.info(f"Table {indexName} schema verified/upgraded successfully")
                return True
            else:
                logger.error(f"Failed to verify/upgrade table {indexName} schema")
                # å¦‚æœå‡çº§å¤±è´¥ï¼Œå°è¯•åˆ é™¤å¹¶é‡æ–°åˆ›å»ºè¡¨
                logger.warning(f"Attempting to drop and recreate table {indexName}")
                try:
                    self.deleteIdx(indexName, knowledgebaseId)
                    logger.info(f"Dropped table {indexName}, will recreate")
                except Exception as e:
                    logger.error(f"Failed to drop table {indexName}: {str(e)}")
                    return False
        
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor() as cur:
                # åˆ›å»ºæ›´å®Œå–„çš„è¡¨ç»“æ„ï¼ŒåŒ…å«ä¸ESå…¼å®¹çš„å­—æ®µ
                cur.execute(f"""
                    CREATE TABLE IF NOT EXISTS {indexName} (
                        id TEXT PRIMARY KEY,
                        kb_id TEXT NOT NULL,
                        doc_id TEXT,                 -- æ–‡æ¡£ID
                        docnm_kwd TEXT,              -- æ–‡æ¡£åç§°(ä¸ESå…¼å®¹)
                        content_ltks TEXT,           -- å†…å®¹æ ‡è®°(ä¸ESå…¼å®¹)
                        content_sm_ltks TEXT,        -- å†…å®¹ç»†ç²’åº¦æ ‡è®°(ä¸ESå…¼å®¹)
                        content_with_weight TEXT,    -- å¸¦æƒé‡å†…å®¹(ä¸ESå…¼å®¹)
                        title_tks TEXT,              -- æ ‡é¢˜æ ‡è®°(ä¸ESå…¼å®¹)
                        title_sm_tks TEXT,           -- æ ‡é¢˜ç»†ç²’åº¦æ ‡è®°(ä¸ESå…¼å®¹)
                        name_kwd TEXT,               -- åç§°å…³é”®è¯(ä¸ESå…¼å®¹)
                        important_kwd JSONB,         -- é‡è¦å…³é”®è¯(ä¸ESå…¼å®¹)
                        important_tks TEXT,          -- é‡è¦å…³é”®è¯æ ‡è®°(ä¸ESå…¼å®¹)
                        question_kwd JSONB,          -- é—®é¢˜å…³é”®è¯(ä¸ESå…¼å®¹)
                        question_tks TEXT,           -- é—®é¢˜å…³é”®è¯æ ‡è®°(ä¸ESå…¼å®¹)
                        img_id TEXT,                 -- å›¾ç‰‡ID(ä¸ESå…¼å®¹)
                        position_int JSONB,          -- ä½ç½®ä¿¡æ¯(ä¸ESå…¼å®¹ï¼Œæ”¯æŒæ•°ç»„)
                        page_num_int JSONB,          -- é¡µç (ä¸ESå…¼å®¹ï¼Œæ”¯æŒæ•°ç»„)
                        top_int JSONB,               -- é¡¶éƒ¨ä½ç½®(ä¸ESå…¼å®¹ï¼Œæ”¯æŒæ•°ç»„)
                        available_int INTEGER DEFAULT 1,  -- å¯ç”¨æ ‡å¿—
                        create_timestamp_flt FLOAT,  -- åˆ›å»ºæ—¶é—´æˆ³(ä¸ESå…¼å®¹)
                        create_time TEXT,            -- åˆ›å»ºæ—¶é—´å­—ç¬¦ä¸²
                        authors_tks TEXT,            -- ä½œè€…æ ‡è®°(ä¸ESå…¼å®¹)
                        authors_sm_tks TEXT,         -- ä½œè€…ç»†ç²’åº¦æ ‡è®°(ä¸ESå…¼å®¹)
                        weight_int JSONB DEFAULT '0',     -- æƒé‡æ•´æ•°(æ”¯æŒæ•°ç»„)
                        weight_flt JSONB DEFAULT '0.0',   -- æƒé‡æµ®ç‚¹(æ”¯æŒæ•°ç»„)
                        rank_int JSONB DEFAULT '0',       -- æ’åæ•´æ•°(æ”¯æŒæ•°ç»„)
                        rank_flt JSONB DEFAULT '0.0',     -- æ’åæµ®ç‚¹(æ”¯æŒæ•°ç»„)
                        knowledge_graph_kwd TEXT,    -- çŸ¥è¯†å›¾è°±å…³é”®è¯
                        entities_kwd TEXT,           -- å®ä½“å…³é”®è¯
                        pagerank_fea INTEGER DEFAULT 0,   -- PageRankç‰¹å¾
                        tag_feas TEXT,               -- æ ‡ç­¾ç‰¹å¾
                        tag_kwd TEXT,                -- æ ‡ç­¾å…³é”®è¯
                        from_entity_kwd TEXT,        -- æ¥æºå®ä½“å…³é”®è¯
                        to_entity_kwd TEXT,          -- ç›®æ ‡å®ä½“å…³é”®è¯
                        entity_kwd TEXT,             -- å®ä½“å…³é”®è¯
                        entity_type_kwd TEXT,        -- å®ä½“ç±»å‹å…³é”®è¯
                        source_id TEXT,              -- æºID
                        n_hop_with_weight TEXT,      -- Nè·³å¸¦æƒé‡
                        removed_kwd TEXT,            -- ç§»é™¤çš„å…³é”®è¯
                        metadata JSONB,              -- å…¶ä»–å…ƒæ•°æ®
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                        embedding vector({vectorSize})
                    )
                """)
                
                # åˆ›å»ºè§¦å‘å™¨è‡ªåŠ¨æ›´æ–° updated_at å­—æ®µ
                cur.execute(f"""
                    CREATE OR REPLACE FUNCTION update_modified_column()
                    RETURNS TRIGGER AS $$
                    BEGIN
                        NEW.updated_at = CURRENT_TIMESTAMP;
                        RETURN NEW;
                    END;
                    $$ language 'plpgsql';
                """)
                
                cur.execute(f"""
                    DROP TRIGGER IF EXISTS update_{indexName}_timestamp ON {indexName};
                    CREATE TRIGGER update_{indexName}_timestamp
                    BEFORE UPDATE ON {indexName}
                    FOR EACH ROW EXECUTE FUNCTION update_modified_column();
                """)
                
                # åˆ›å»º kb_id ç´¢å¼• - æé«˜æŒ‰çŸ¥è¯†åº“è¿‡æ»¤æ€§èƒ½
                cur.execute(f"""
                    CREATE INDEX IF NOT EXISTS {indexName}_kb_id_idx ON {indexName} (kb_id);
                """)
                
                # åˆ›å»ºBM25ç´¢å¼• - ç”¨äºå…¨æ–‡æœç´¢
                cur.execute(f"""
                    CREATE INDEX IF NOT EXISTS {indexName}_bm25 ON {indexName}
                    USING bm25 (
                        id, 
                        content_with_weight, 
                        content_ltks, 
                        content_sm_ltks,
                        title_tks,
                        title_sm_tks,
                        important_tks,
                        question_tks,
                        kb_id, 
                        available_int, 
                        metadata
                    ) WITH (key_field='id');
                """)
                
                # åˆ›å»ºå‘é‡ç´¢å¼• - ç”¨äºå‘é‡ç›¸ä¼¼åº¦æœç´¢
                cur.execute(f"""
                    CREATE INDEX IF NOT EXISTS {indexName}_vector_idx 
                    ON {indexName} 
                    USING hnsw (embedding vector_cosine_ops)
                    WITH (
                        m = 16,               -- æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°
                        ef_construction = 64  -- æ„å»ºæ—¶çš„æœç´¢å®½åº¦ï¼Œè¶Šå¤§è¶Šç²¾ç¡®ä½†ä¹Ÿè¶Šæ…¢
                    )
                """)
                
                conn.commit()
                logger.info(f"Creating index with name: {indexName}")
                # åˆ›å»ºåéªŒè¯
                if self.indexExist(indexName):
                    logger.info(f"Successfully created and verified index: {indexName}")
                else:
                    logger.error(f"Failed to verify index after creation: {indexName}")
                return True
        except Exception as e:
            logger.exception(f"PDConnection.createIdx error {indexName}: {str(e)}")
            return False
        finally:
            if conn:
                self._return_connection(conn)

    def deleteIdx(self, indexName: str, knowledgebaseId: str):
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor() as cur:
                # åˆ é™¤BM25ç´¢å¼•
                cur.execute(f"DROP INDEX IF EXISTS {indexName}_bm25")
                # åˆ é™¤å‘é‡ç´¢å¼•
                cur.execute(f"DROP INDEX IF EXISTS {indexName}_vector_idx")
                # åˆ é™¤è¡¨
                cur.execute(f"DROP TABLE IF EXISTS {indexName}")
                conn.commit()
        except Exception as e:
            logger.exception(f"PDConnection.deleteIdx error {indexName}: {str(e)}")
        finally:
            if conn:
                self._return_connection(conn)

    def indexExist(self, indexName: str, knowledgebaseId: str = None) -> bool:
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = %s
                    )
                """, (indexName,))
                return cur.fetchone()[0]
        except Exception as e:
            logger.exception(f"PDConnection.indexExist got exception: {str(e)}")
            return False
        finally:
            if conn:
                self._return_connection(conn)

    """
    CRUD operations
    """

    def search(
            self, selectFields: list[str],
            highlightFields: list[str],
            condition: dict,
            matchExprs: list[MatchExpr],
            orderBy: OrderByExpr,
            offset: int,
            limit: int,
            indexNames: str | list[str],
            knowledgebaseIds: list[str],
            aggFields: list[str] = [],
            rank_feature: dict | None = None
    ):
        if isinstance(indexNames, str):
            indexNames = indexNames.split(",")

        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›ç©ºç»“æœ
        if not self.indexExist(indexNames[0]):
            logger.warning(f"Table {indexNames[0]} does not exist, returning empty result")
            return {
                "hits": {
                    "total": 0,
                    "hits": []
                }
            }

        # æ„å»ºSELECTå­å¥ - å¤„ç†å‘é‡å­—æ®µæ˜ å°„
        select_clause = []
        vector_field_pattern = re.compile(r'q_(\d+)_vec')
        for field in selectFields:
            # å¦‚æœæ˜¯åŠ¨æ€å‘é‡å­—æ®µï¼Œæ˜ å°„åˆ°embeddingå­—æ®µ
            if vector_field_pattern.match(field):
                select_clause.append("embedding")
                logger.debug(f"Mapped field {field} to embedding in SELECT clause")
            else:
                select_clause.append(field)

        # æ·»åŠ é«˜äº®
        for field in highlightFields:
            select_clause.append(f"paradedb.snippet({field}) as {field}_hl")

        # æ„å»ºWHEREå­å¥
        where_conditions = []
        params = []
        if knowledgebaseIds:
            where_conditions.append(f"kb_id = ANY(%s)")
            params.append(knowledgebaseIds)

        # å¤„ç†conditionå‚æ•° - æ·»åŠ è¿‡æ»¤æ¡ä»¶
        for key, value in condition.items():
            if not value:
                continue
            
            # è·³è¿‡éè¿‡æ»¤æ¡ä»¶çš„å‚æ•°
            if key in ["vector_similarity_weight"]:
                continue
                
            if key == "available_int":
                if value == 0:
                    where_conditions.append(f"available_int < 1")
                else:
                    where_conditions.append(f"available_int >= 1")
            elif isinstance(value, list):
                where_conditions.append(f"{key} = ANY(%s)")
                params.append(value)
            elif isinstance(value, (str, int)):
                where_conditions.append(f"{key} = %s")
                params.append(value)
            else:
                logger.warning(f"Unsupported condition type for {key}: {type(value)}")

        # å¤„ç†åŒ¹é…è¡¨è¾¾å¼
        has_vector_search = False
        vector_query = None
        vector_topn = 10
        vector_similarity_weight = 0.5  # é»˜è®¤æƒé‡
        has_text_search = False
        text_query = None

        # é¦–å…ˆä»conditionä¸­æ£€æŸ¥æ˜¯å¦æœ‰vector_similarity_weightå‚æ•°
        if "vector_similarity_weight" in condition:
            try:
                vector_similarity_weight = float(condition["vector_similarity_weight"])
                logger.debug(f"Using vector_similarity_weight from condition: {vector_similarity_weight}")
            except (ValueError, TypeError):
                logger.warning(f"Invalid vector_similarity_weight in condition: {condition['vector_similarity_weight']}, using default: {vector_similarity_weight}")

        # ç„¶åæ£€æŸ¥æ˜¯å¦æœ‰FusionExprå¹¶æå–æƒé‡ï¼ˆFusionExprä¼˜å…ˆçº§æ›´é«˜ï¼‰
        for m in matchExprs:
            if isinstance(m, FusionExpr) and m.method == "weighted_sum" and "weights" in m.fusion_params:
                # å‡è®¾åŒ¹é…è¡¨è¾¾å¼é¡ºåºæ˜¯ï¼š[MatchTextExpr, MatchDenseExpr, FusionExpr]
                weights = m.fusion_params["weights"]
                # è§£ææƒé‡å­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º: "æ–‡æœ¬æƒé‡, å‘é‡æƒé‡"
                weight_parts = weights.split(",")
                if len(weight_parts) >= 2:
                    try:
                        # æ–‡æœ¬æœç´¢æƒé‡
                        text_weight = float(weight_parts[0].strip())
                        # å‘é‡æœç´¢æƒé‡
                        vector_similarity_weight = float(weight_parts[1].strip())
                        logger.info(f"ğŸ¯ Using weights from FusionExpr: text={text_weight:.3f}, vector={vector_similarity_weight:.3f}")
                    except ValueError:
                        logger.warning(f"Invalid weight format: {weights}, using default weight: text=0.5, vector={vector_similarity_weight}")

        # ç„¶åå¤„ç†å„ç§åŒ¹é…è¡¨è¾¾å¼
        for expr in matchExprs:
            if isinstance(expr, MatchTextExpr):
                has_text_search = True
                text_query = expr.matching_text
                
                # å¤„ç†å­—æ®µçš„boostè¯­æ³• - å°† ES çš„ field^boost è½¬æ¢ä¸º ParadeDB å…¼å®¹æ ¼å¼
                # ParadeDB ä¸ç›´æ¥æ”¯æŒå­—æ®µçº§åˆ«çš„ boostï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡å­—æ®µä¼˜å…ˆçº§æ¥æ¨¡æ‹Ÿ
                field_weights = []
                for field in expr.fields:
                    if "^" in field:
                        field_name, boost_str = field.split("^", 1)
                        try:
                            boost_value = float(boost_str)
                            field_weights.append((field_name, boost_value))
                        except ValueError:
                            field_weights.append((field_name, 1.0))
                            logger.warning(f"Invalid boost value in field: {field}, using default boost 1.0")
                    else:
                        field_weights.append((field, 1.0))
                
                # æŒ‰æƒé‡æ’åºï¼Œä¼˜å…ˆä½¿ç”¨é«˜æƒé‡å­—æ®µ
                field_weights.sort(key=lambda x: x[1], reverse=True)
                
                # æ„å»ºåˆ†å±‚æœç´¢ç­–ç•¥ï¼šä¼˜å…ˆåŒ¹é…é«˜æƒé‡å­—æ®µï¼Œå¦‚æœæ²¡æœ‰ç»“æœå†æ‰©å±•åˆ°ä½æƒé‡å­—æ®µ
                high_priority_fields = [f for f, w in field_weights if w >= 10.0]  # é«˜ä¼˜å…ˆçº§å­—æ®µ
                medium_priority_fields = [f for f, w in field_weights if 2.0 <= w < 10.0]  # ä¸­ä¼˜å…ˆçº§å­—æ®µ
                low_priority_fields = [f for f, w in field_weights if w < 2.0]  # ä½ä¼˜å…ˆçº§å­—æ®µ
                
                # å¯¹æœç´¢æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œä¸ESä¿æŒä¸€è‡´
                tokenized_search_text = rag_tokenizer.fine_grained_tokenize(rag_tokenizer.tokenize(expr.matching_text))
                
                # æ˜ å°„ESå­—æ®µååˆ°ParadeDBå®é™…å­—æ®µå
                field_mapping = {
                    'title_tks': 'title_tks',
                    'title_sm_tks': 'title_sm_tks', 
                    'important_kwd': 'important_kwd',
                    'important_tks': 'important_tks',
                    'question_tks': 'question_tks',
                    'content_ltks': 'content_ltks',
                    'content_sm_ltks': 'content_sm_ltks',
                    'content_with_weight': 'content_with_weight',
                    'content': 'content_with_weight',  # æ˜ å°„contentåˆ°content_with_weight
                    'title': 'title_tks'  # æ˜ å°„titleåˆ°title_tks
                }
                
                # å°†å­—æ®µåæ˜ å°„åˆ°å®é™…å­˜åœ¨çš„å­—æ®µ
                mapped_field_weights = []
                for field_name, boost_value in field_weights:
                    mapped_field = field_mapping.get(field_name, field_name)
                    mapped_field_weights.append((mapped_field, boost_value))
                    if field_name != mapped_field:
                        logger.debug(f"Mapped search field {field_name} -> {mapped_field}")
                
                # é‡æ–°æŒ‰æƒé‡æ’åº
                mapped_field_weights.sort(key=lambda x: x[1], reverse=True)
                
                # æ„å»ºåˆ†å±‚æœç´¢ç­–ç•¥ï¼šä¼˜å…ˆåŒ¹é…é«˜æƒé‡å­—æ®µ
                high_priority_fields = [f for f, w in mapped_field_weights if w >= 10.0]
                medium_priority_fields = [f for f, w in mapped_field_weights if 2.0 <= w < 10.0]
                low_priority_fields = [f for f, w in mapped_field_weights if w < 2.0]
                
                # æ„å»ºæœç´¢æ¡ä»¶ - ä½¿ç”¨åµŒå¥—çš„ä¼˜å…ˆçº§ç»“æ„
                search_conditions = []
                
                if high_priority_fields:
                    high_conditions = []
                    for field in high_priority_fields:
                        high_conditions.append(f"{field} @@@ %s")
                        params.append(tokenized_search_text)
                    search_conditions.append(f"({' OR '.join(high_conditions)})")
                
                if medium_priority_fields:
                    medium_conditions = []
                    for field in medium_priority_fields:
                        medium_conditions.append(f"{field} @@@ %s")
                        params.append(tokenized_search_text)
                    search_conditions.append(f"({' OR '.join(medium_conditions)})")
                
                if low_priority_fields:
                    low_conditions = []
                    for field in low_priority_fields:
                        low_conditions.append(f"{field} @@@ %s")
                        params.append(tokenized_search_text)
                    search_conditions.append(f"({' OR '.join(low_conditions)})")
                
                # å°†æ‰€æœ‰ä¼˜å…ˆçº§çš„æ¡ä»¶ç”¨ OR è¿æ¥ï¼Œä½†é«˜ä¼˜å…ˆçº§å­—æ®µä¼šå› ä¸º BM25 ç®—æ³•è‡ªç„¶è·å¾—æ›´é«˜åˆ†æ•°
                if search_conditions:
                    where_conditions.append(f"({' OR '.join(search_conditions)})")
                
                logger.debug(f"Text search field priorities: high={high_priority_fields}, medium={medium_priority_fields}, low={low_priority_fields}")
            elif isinstance(expr, MatchDenseExpr):
                # æ ‡è®°å­˜åœ¨å‘é‡æœç´¢ï¼Œç¨ååœ¨ORDER BYä¸­å¤„ç†
                has_vector_search = True
                vector_query = expr.embedding_data
                vector_topn = expr.topn
                # ParadeDBä¸­ä½¿ç”¨å›ºå®šçš„embeddingå­—æ®µåï¼Œæ— è®ºåŸå§‹å­—æ®µåæ˜¯ä»€ä¹ˆ
                # åŸå§‹å­—æ®µåå¦‚q_1024_vecä¼šåœ¨insertæ—¶æ˜ å°„åˆ°embeddingå­—æ®µ
                logger.info(f"Vector search requested with field: {expr.vector_column_name}, mapping to embedding field, topn: {vector_topn}")

        # æ„å»ºWHEREå­å¥å­—ç¬¦ä¸²
        where_clause = ""
        if where_conditions:
            where_clause = "WHERE " + " AND ".join(where_conditions)

        # æ„å»ºORDER BYå­å¥
        order_clause = []

        # æ··åˆæœç´¢æ’åºç­–ç•¥
        # ä¸ESè¿æ¥ä¿æŒä¸€è‡´ï¼šæ— è®ºæƒé‡å¦‚ä½•éƒ½åŒæ—¶è€ƒè™‘æ–‡æœ¬æœç´¢å’Œå‘é‡æœç´¢
        # å‚è€ƒESè¿æ¥å®ç°æ–¹å¼: bqry.boost = 1.0 - vector_similarity_weight
        if has_vector_search and has_text_search:
            # è®¡ç®—æ–‡æœ¬æœç´¢æƒé‡
            text_weight = 1.0 - vector_similarity_weight
            
            # åœ¨WHEREå­å¥ä¸­å·²ç»æ·»åŠ äº†æ–‡æœ¬æœç´¢æ¡ä»¶
            # åœ¨ORDER BYä¸­æ·»åŠ å‘é‡æœç´¢æ’åºï¼Œç¡®ä¿ä¸¤ç§æœç´¢æ–¹å¼éƒ½è¢«ä½¿ç”¨
            # å°† Python åˆ—è¡¨è½¬æ¢ä¸º ParadeDB å‘é‡æ ¼å¼
            vector_str = '[' + ','.join(map(str, vector_query)) + ']'
            order_clause = [f"embedding <=> '{vector_str}'::vector"]
            logger.info(f"ğŸ” Using hybrid search with weights: text={text_weight:.3f}, vector={vector_similarity_weight:.3f}")
        elif has_vector_search:
            # åªæœ‰å‘é‡æœç´¢
            vector_str = '[' + ','.join(map(str, vector_query)) + ']'
            order_clause = [f"embedding <=> '{vector_str}'::vector"]
            logger.info(f"ğŸ” Using vector search only with weight={vector_similarity_weight:.3f}")
        elif has_text_search:
            # åªæœ‰æ–‡æœ¬æœç´¢ï¼ŒParadeDBé»˜è®¤ä½¿ç”¨BM25æ’åº
            logger.info("ğŸ” Using text search only with BM25 ranking")

        # å¦‚æœè¿˜æœ‰å…¶ä»–æ’åºæ¡ä»¶ï¼Œæ·»åŠ åˆ°åé¢
        if orderBy and orderBy.fields:
            for field, order in orderBy.fields:
                order_clause.append(f"{field} {'DESC' if order == 1 else 'ASC'}")
        
        # æ„å»ºå®Œæ•´SQL
        # ç¡®ä¿SQLè¯­æ³•æ­£ç¡®
        select_part = f"SELECT {', '.join(select_clause)}"
        from_part = f"FROM {indexNames[0]}"
        
        # æ·»åŠ é™åˆ¶æ¡ä»¶
        limit_clause = ""
        if has_vector_search:
            limit_clause = f"LIMIT {vector_topn}"
        elif limit > 0:
            limit_clause = f"LIMIT {limit}"
            
        offset_clause = f"OFFSET {offset}" if offset > 0 else ""
        order_by_clause = f"ORDER BY {', '.join(order_clause)}" if order_clause else ""
        
        # æ„å»ºå®Œæ•´SQL - ç¡®ä¿æ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰æ­£ç¡®çš„å…³é”®å­—
        sql = f"""
            {select_part}
            {from_part}
            {where_clause}
            {order_by_clause}
            {limit_clause}
            {offset_clause}
        """
        
        logger.debug(f"Generated SQL: {sql}")

        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                # æ‰§è¡Œä¸»æŸ¥è¯¢
                cur.execute(sql, params)
                raw_results = cur.fetchall()
                
                logger.debug(f"PDConnection.search raw results count: {len(raw_results)}")
                if raw_results:
                    logger.debug(f"First result keys: {list(raw_results[0].keys())}")
                    logger.debug(f"First result sample: {dict(raw_results[0])}")
                
                # å°†ParadeDBç»“æœæ ¼å¼è½¬æ¢ä¸ºESå…¼å®¹æ ¼å¼
                formatted_hits = []
                for i, row in enumerate(raw_results):
                    row_dict = dict(row)
                    # æå–idä½œä¸º_idï¼Œä½†ä¿ç•™åœ¨_sourceä¸­
                    doc_id = row_dict.get('id', '')
                    
                    logger.debug(f"Row {i}: doc_id='{doc_id}', content_preview='{str(row_dict.get('content_with_weight', ''))[:50]}...'")
                    
                    # å¦‚æœidä¸ºç©ºï¼Œç”Ÿæˆå”¯ä¸€æ ‡è¯†
                    if not doc_id:
                        logger.warning(f"Empty doc_id found in row {i}, generating fallback ID")
                        # æ ¹æ®å†…å®¹ç”Ÿæˆhashä½œä¸ºä¸´æ—¶ID
                        content_hash = hashlib.md5(str(row_dict.get('content_with_weight', f'row_{i}')).encode()).hexdigest()
                        doc_id = f"temp_{content_hash[:16]}"
                        row_dict['id'] = doc_id  # å°†ç”Ÿæˆçš„IDä¹Ÿæ”¾å…¥_sourceä¸­
                        logger.debug(f"Generated fallback doc_id: {doc_id}")
                    
                    # å°†embeddingå­—æ®µæ˜ å°„å›åŠ¨æ€çš„å‘é‡å­—æ®µåä»¥ä¿æŒå…¼å®¹æ€§
                    if 'embedding' in row_dict and row_dict['embedding'] is not None:
                        vector_data = row_dict.pop('embedding')
                        # æ ¹æ®å‘é‡é•¿åº¦ç”ŸæˆåŠ¨æ€å­—æ®µå
                        if isinstance(vector_data, list):
                            vector_size = len(vector_data)
                        else:
                            try:
                                # å¤„ç†PostgreSQLå‘é‡ç±»å‹
                                if hasattr(vector_data, 'tolist'):
                                    vector_data = vector_data.tolist()
                                else:
                                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼çš„å‘é‡ï¼Œå°è¯•è§£æ
                                    if isinstance(vector_data, str):
                                        # ç§»é™¤æ–¹æ‹¬å·å¹¶åˆ†å‰²
                                        vector_str = vector_data.strip('[]')
                                        vector_data = [float(x.strip()) for x in vector_str.split(',')]
                                    else:
                                        vector_data = list(vector_data)
                                vector_size = len(vector_data)
                            except Exception as e:
                                logger.warning(f"Failed to parse vector data: {e}")
                                vector_size = 1024  # é»˜è®¤å‘é‡å¤§å°
                                vector_data = [0.0] * vector_size
                        
                        # æ£€æŸ¥selectFieldsä¸­æ˜¯å¦æœ‰å¯¹åº”çš„q_*_vecå­—æ®µè¯·æ±‚
                        target_field = f'q_{vector_size}_vec'
                        for field in selectFields:
                            if vector_field_pattern.match(field):
                                target_field = field
                                break
                        
                        row_dict[target_field] = vector_data
                        logger.debug(f"Mapped embedding back to {target_field} with {len(vector_data)} dimensions")
                    
                    # æ„å»ºESå…¼å®¹çš„hitæ ¼å¼
                    formatted_hit = {
                        "_id": doc_id,
                        "_source": row_dict,
                        "_score": 1.0  # é»˜è®¤è¯„åˆ†ï¼Œå¦‚æœéœ€è¦çœŸå®è¯„åˆ†å¯ä»¥åç»­æ”¹è¿›
                    }
                    formatted_hits.append(formatted_hit)

                results = {
                    "hits": {
                        "total": {"value": len(formatted_hits), "relation": "eq"},
                        "hits": formatted_hits
                    }
                }

                # å¦‚æœéœ€è¦èšåˆå­—æ®µï¼Œæ‰§è¡ŒèšåˆæŸ¥è¯¢
                if aggFields:
                    results["aggregations"] = {}
                    
                    for field in aggFields:
                        # æ„å»ºèšåˆæŸ¥è¯¢
                        agg_sql = f"""
                            SELECT {field}, COUNT(*) as doc_count
                            FROM {indexNames[0]}
                            {where_clause}
                            GROUP BY {field}
                            ORDER BY doc_count DESC
                        """
                        
                        # æ‰§è¡ŒèšåˆæŸ¥è¯¢
                        cur.execute(agg_sql, params[:-1] if has_vector_search else params)  # æ’é™¤å‘é‡æŸ¥è¯¢å‚æ•°
                        
                        # æ„å»ºä¸ESå…¼å®¹çš„èšåˆç»“æœæ ¼å¼
                        buckets = []
                        for row in cur.fetchall():
                            if row[field] is not None:  # å¿½ç•¥NULLå€¼
                                buckets.append({
                                    "key": row[field],
                                    "doc_count": row["doc_count"]
                                })
                        
                        # å°†èšåˆç»“æœæ·»åŠ åˆ°å“åº”ä¸­
                        results["aggregations"][f"aggs_{field}"] = {
                            "buckets": buckets
                        }
                
                return results
        except psycopg2.errors.UndefinedColumn as e:
            # å­—æ®µä¸å­˜åœ¨çš„å‹å¥½é”™è¯¯å¤„ç†
            missing_field = str(e).split('"')[1] if '"' in str(e) else "unknown"
            logger.error(f"Missing column in ParadeDB: {missing_field}")
            return {
                "hits": {
                    "total": 0,
                    "hits": []
                },
                "error": f"å­—æ®µ '{missing_field}' ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“ç»“æ„æˆ–æ›´æ–°åº”ç”¨"
            }
        except Exception as e:
            logger.exception(f"PDConnection.search error: {str(e)}")
            raise e
        finally:
            # ç¡®ä¿è¿æ¥è¢«å½’è¿˜åˆ°è¿æ¥æ± 
            if conn:
                self._return_connection(conn)

    def get(self, chunkId: str, indexName: str, knowledgebaseIds: list[str]) -> dict | None:
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                cur.execute(f"""
                    SELECT * FROM {indexName} 
                    WHERE id = %s AND kb_id = ANY(%s)
                """, (chunkId, knowledgebaseIds))
                result = cur.fetchone()
                if result:
                    result_dict = dict(result)
                    
                    # å°†embeddingå­—æ®µæ˜ å°„å›åŠ¨æ€çš„å‘é‡å­—æ®µåä»¥ä¿æŒå…¼å®¹æ€§
                    if 'embedding' in result_dict and result_dict['embedding'] is not None:
                        vector_data = result_dict.pop('embedding')
                        # æ ¹æ®å‘é‡é•¿åº¦ç”ŸæˆåŠ¨æ€å­—æ®µå
                        vector_size = len(vector_data) if isinstance(vector_data, list) else len(vector_data.tolist())
                        result_dict[f'q_{vector_size}_vec'] = vector_data
                    
                    return result_dict
                return None
        except Exception as e:
            logger.exception(f"PDConnection.get({chunkId}) got exception: {str(e)}")
            return None
        finally:
            if conn:
                self._return_connection(conn)

    def insert(self, documents: list[dict], indexName: str, knowledgebaseId: str = None) -> list[str]:
        errors = []
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor() as cur:
                for doc in documents:
                    try:
                        # ç¡®ä¿æ–‡æ¡£æœ‰idå­—æ®µ
                        if "id" not in doc and "doc_id" in doc:
                            doc["id"] = doc["doc_id"]
                            
                        # åˆ›å»ºæ–‡æ¡£å‰¯æœ¬è€Œä¸æ˜¯ä¿®æ”¹åŸå§‹æ–‡æ¡£
                        doc_copy = copy.deepcopy(doc)
                        
                        # ä»å‰¯æœ¬ä¸­æå–id
                        doc_id = doc_copy.pop("id", "")
                        doc_copy["kb_id"] = knowledgebaseId
                        
                        # å¤„ç†åŠ¨æ€å‘é‡å­—æ®µåæ˜ å°„åˆ°å›ºå®šçš„embeddingå­—æ®µ
                        vector_field_pattern = re.compile(r'q_(\d+)_vec')
                        embedding_data = None
                        for field_name in list(doc_copy.keys()):
                            match = vector_field_pattern.match(field_name)
                            if match:
                                # æ‰¾åˆ°å‘é‡å­—æ®µï¼Œå°†å…¶æ˜ å°„åˆ°embeddingå­—æ®µ
                                embedding_data = doc_copy.pop(field_name)
                                break
                        
                        # å¦‚æœæ‰¾åˆ°äº†å‘é‡æ•°æ®ï¼Œæ·»åŠ åˆ°embeddingå­—æ®µ
                        if embedding_data is not None:
                            doc_copy["embedding"] = embedding_data
                        
                        # å¤„ç†å¯èƒ½åŒ…å«æ•°ç»„æˆ–å¤æ‚æ•°æ®çš„å­—æ®µï¼Œç¡®ä¿å®ƒä»¬èƒ½æ­£ç¡®å­˜å‚¨åˆ°JSONBå­—æ®µä¸­
                        jsonb_fields = {'position_int', 'page_num_int', 'top_int', 'weight_int', 'weight_flt', 'rank_int', 'rank_flt'}
                        for field_name in jsonb_fields:
                            if field_name in doc_copy:
                                value = doc_copy[field_name]
                                # å¦‚æœå€¼ä¸æ˜¯Noneä¸”ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œå°†å…¶è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                                if value is not None and not isinstance(value, str):
                                    try:
                                        import json
                                        doc_copy[field_name] = json.dumps(value)
                                    except (TypeError, ValueError) as e:
                                        # å¦‚æœæ— æ³•åºåˆ—åŒ–ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                        doc_copy[field_name] = str(value)
                                        logger.warning(f"Field {field_name} value {value} converted to string: {e}")
                        
                        # æ„å»ºSQLçš„å­—æ®µåˆ—è¡¨å’Œå ä½ç¬¦
                        fields = ", ".join(doc_copy.keys())
                        placeholders = ", ".join(["%s"] * len(doc_copy))
                        
                        # æ„å»ºUPDATEéƒ¨åˆ†
                        update_clause = ", ".join([f"{k} = EXCLUDED.{k}" for k in doc_copy.keys()])

                        sql = f"""
                            INSERT INTO {indexName} (id, {fields})
                            VALUES (%s, {placeholders})
                            ON CONFLICT (id) DO UPDATE
                            SET {update_clause}
                        """

                        cur.execute(sql, [doc_id] + list(doc_copy.values()))
                    except Exception as e:
                        errors.append(f"{doc.get('id', 'unknown')}:{str(e)}")
                        logger.error(f"Insert error for doc {doc.get('id', 'unknown')}: {str(e)}")
                        conn.rollback()
                        continue

                conn.commit()
            return errors
        except Exception as e:
            logger.exception(f"PDConnection.insert got exception: {str(e)}")
            return [str(e)]
        finally:
            if conn:
                self._return_connection(conn)

    def update(self, condition: dict, newValue: dict, indexName: str, knowledgebaseId: str) -> bool:
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor() as cur:
                # å¤„ç†ESå…¼å®¹çš„ç‰¹æ®Šæ“ä½œ
                if "remove" in newValue:
                    return self._handle_remove_operation(condition, newValue, indexName, knowledgebaseId, cur, conn)
                
                # æ„å»ºSETå­å¥
                set_clause = []
                params = []
                
                # å¤„ç†newValueä¸­çš„æ•°æ®ç±»å‹ï¼Œç¡®ä¿psycopg2èƒ½å¤Ÿå¤„ç†
                for k, v in newValue.items():
                    set_clause.append(f"{k} = %s")
                    
                    # å¤„ç†ä¸åŒç±»å‹çš„å€¼
                    if v is None:
                        params.append(None)
                    elif isinstance(v, (str, int, float, bool)):
                        params.append(v)
                    elif isinstance(v, (dict, list)):
                        # å°†å­—å…¸å’Œåˆ—è¡¨è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                        import json
                        try:
                            params.append(json.dumps(v, ensure_ascii=False))
                        except (TypeError, ValueError) as e:
                            logger.warning(f"Failed to serialize {k}={v} to JSON: {e}, using string representation")
                            params.append(str(v))
                    else:
                        # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        params.append(str(v))
                        logger.debug(f"Converted {k}={v} (type: {type(v)}) to string")

                # æ„å»ºWHEREå­å¥
                where_conditions = []
                params_where = []
                
                # å¤„ç†knowledgebaseId
                if knowledgebaseId:
                    where_conditions.append("kb_id = %s")
                    params_where.append(knowledgebaseId)

                # å¤„ç†å…¶ä»–æ¡ä»¶
                for key, value in condition.items():
                    if key == "kb_id":
                        continue  # å·²ç»å¤„ç†è¿‡äº†
                    elif key == "id":
                        where_conditions.append("id = %s")
                        params_where.append(value)
                    elif key == "knowledge_graph_kwd":
                        if isinstance(value, list):
                            # å¤„ç†çŸ¥è¯†å›¾è°±å…³é”®è¯æ•°ç»„æŸ¥è¯¢
                            placeholders = ",".join(["%s"] * len(value))
                            where_conditions.append(f"knowledge_graph_kwd = ANY(ARRAY[{placeholders}])")
                            params_where.extend(value)
                        else:
                            where_conditions.append("knowledge_graph_kwd = %s")
                            params_where.append(value)
                    elif key == "source_id":
                        where_conditions.append("source_id = %s")
                        params_where.append(value)
                    elif isinstance(value, list):
                        where_conditions.append(f"{key} = ANY(%s)")
                        params_where.append(value)
                    elif isinstance(value, (str, int)):
                        where_conditions.append(f"{key} = %s")
                        params_where.append(value)
                    else:
                        logger.warning(f"Unsupported condition type for {key}: {type(value)}")

                if not set_clause:
                    logger.warning("No valid fields to update")
                    return True

                sql = f"""
                    UPDATE {indexName}
                    SET {', '.join(set_clause)}
                    WHERE {' AND '.join(where_conditions)}
                """

                # åˆå¹¶å‚æ•°ï¼šå…ˆSETå‚æ•°ï¼ŒåWHEREå‚æ•°
                all_params = params + params_where
                
                logger.debug(f"Update SQL: {sql}")
                logger.debug(f"Update params: {all_params}")
                
                cur.execute(sql, all_params)
                affected_rows = cur.rowcount
                conn.commit()
                
                logger.debug(f"Updated {affected_rows} rows")
                return True
        except Exception as e:
            logger.exception(f"PDConnection.update got exception: {str(e)}")
            return False
        finally:
            if conn:
                self._return_connection(conn)

    def _handle_remove_operation(self, condition: dict, newValue: dict, indexName: str, knowledgebaseId: str, cur, conn):
        """å¤„ç†ESå…¼å®¹çš„removeæ“ä½œ"""
        try:
            remove_spec = newValue["remove"]
            logger.debug(f"Handling remove operation: {remove_spec}")
            
            if isinstance(remove_spec, dict):
                # å¤„ç†å­—å…¸å½¢å¼çš„removeæ“ä½œï¼Œå¦‚ {"source_id": "some_id"}
                for field_name, field_value in remove_spec.items():
                    if field_name == "source_id":
                        # å¯¹äºsource_idå­—æ®µï¼Œå°†å…¶è®¾ç½®ä¸ºNULLæˆ–åˆ é™¤å¼•ç”¨
                        update_sql = f"""
                            UPDATE {indexName}
                            SET source_id = NULL
                            WHERE source_id = %s
                        """
                        if knowledgebaseId:
                            update_sql += " AND kb_id = %s"
                            cur.execute(update_sql, [field_value, knowledgebaseId])
                        else:
                            cur.execute(update_sql, [field_value])
                        
                        logger.debug(f"Removed source_id reference: {field_value}")
                    else:
                        # å¯¹äºå…¶ä»–å­—æ®µï¼Œå°è¯•ä»JSONBæ•°ç»„ä¸­ç§»é™¤å…ƒç´ 
                        logger.warning(f"Remove operation for field {field_name} not fully implemented, setting to NULL")
                        update_sql = f"""
                            UPDATE {indexName}
                            SET {field_name} = NULL
                            WHERE {field_name} = %s
                        """
                        if knowledgebaseId:
                            update_sql += " AND kb_id = %s"
                            cur.execute(update_sql, [field_value, knowledgebaseId])
                        else:
                            cur.execute(update_sql, [field_value])
            
            elif isinstance(remove_spec, str):
                # å¤„ç†å­—ç¬¦ä¸²å½¢å¼çš„removeæ“ä½œï¼Œç›´æ¥åˆ é™¤å­—æ®µ
                logger.warning(f"String remove operation for field {remove_spec} not implemented, skipping")
            
            # å¤„ç†å…¶ä»–newValueä¸­çš„å­—æ®µ
            other_updates = {k: v for k, v in newValue.items() if k != "remove"}
            if other_updates:
                # é€’å½’è°ƒç”¨updateæ–¹æ³•å¤„ç†å…¶ä»–å­—æ®µ
                return self.update(condition, other_updates, indexName, knowledgebaseId)
            
            conn.commit()
            return True
            
        except Exception as e:
            logger.exception(f"Error handling remove operation: {str(e)}")
            return False

    def delete(self, condition: dict, indexName: str, knowledgebaseId: str) -> int:
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor() as cur:
                where_conditions = ["kb_id = %s"]
                params = [knowledgebaseId]

                if "id" in condition:
                    chunk_ids = condition["id"]
                    if not isinstance(chunk_ids, list):
                        chunk_ids = [chunk_ids]
                    if chunk_ids:
                        where_conditions.append("id = ANY(%s)")
                        params.append(chunk_ids)

                sql = f"""
                    DELETE FROM {indexName}
                    WHERE {' AND '.join(where_conditions)}
                    RETURNING id
                """

                cur.execute(sql, params)
                deleted = cur.rowcount
                conn.commit()
                return deleted
        except Exception as e:
            logger.exception(f"PDConnection.delete got exception: {str(e)}")
            return 0
        finally:
            if conn:
                self._return_connection(conn)

    """
    Helper functions for search result
    """

    def getTotal(self, res):
        if isinstance(res["hits"]["total"], dict):
            return res["hits"]["total"]["value"]
        return res["hits"]["total"]

    def getChunkIds(self, res):
        return [hit["_id"] for hit in res["hits"]["hits"]]

    def getFields(self, res, fields: list[str]) -> dict[str, dict]:
        result = {}
        for hit in res["hits"]["hits"]:
            doc_id = hit["_id"]
            source_data = hit["_source"]
            field_values = {}
            
            for field in fields:
                if field in source_data:
                    field_values[field] = source_data[field]
                else:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å‘é‡å­—æ®µè¯·æ±‚ï¼Œéœ€è¦ä»embeddingå­—æ®µæ˜ å°„
                    vector_field_pattern = re.compile(r'q_(\d+)_vec')
                    if vector_field_pattern.match(field):
                        # è¯·æ±‚çš„æ˜¯åŠ¨æ€å‘é‡å­—æ®µï¼Œä»embeddingå­—æ®µè·å–
                        if "embedding" in source_data and source_data["embedding"] is not None:
                            field_values[field] = source_data["embedding"]
                        else:
                            # å¦‚æœembeddingå­—æ®µä¹Ÿæ²¡æœ‰ï¼Œè®¾ç½®ä¸ºç©ºå‘é‡
                            vector_size = int(vector_field_pattern.match(field).group(1))
                            field_values[field] = [0.0] * vector_size
            
            if field_values:
                result[doc_id] = field_values
        return result

    def getHighlight(self, res, keywords: list[str], fieldnm: str):
        highlights = {}
        for hit in res["hits"]["hits"]:
            doc_id = hit["_id"]
            source_data = hit["_source"]
            hl_field = f"{fieldnm}_hl"
            if hl_field in source_data:
                highlights[doc_id] = source_data[hl_field]
        return highlights

    def getAggregation(self, res, fieldnm: str):
        """
        ä»æœç´¢ç»“æœä¸­æå–ç‰¹å®šå­—æ®µçš„èšåˆç»“æœ
        """
        agg_field = "aggs_" + fieldnm
        if "aggregations" not in res or agg_field not in res["aggregations"]:
            return list()
        
        buckets = res["aggregations"][agg_field]["buckets"]
        return [(b["key"], b["doc_count"]) for b in buckets]

    """
    SQL æ‰§è¡Œæ–¹æ³•
    """
    def sql(self, sql: str, fetch_size: int, format: str):
        """
        æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
        """
        logger.debug(f"PDConnection.sql get sql: {sql}")
        
        # SQLé¢„å¤„ç† - ç±»ä¼¼ESå®ç°çš„è§„èŒƒåŒ–
        sql = re.sub(r"[ `]+", " ", sql)
        sql = sql.replace("%", "")
        
        # å¤„ç†å‘é‡å­—æ®µåæ˜ å°„
        # å°†q_*_vecå­—æ®µæ˜ å°„åˆ°embeddingå­—æ®µ
        vector_field_pattern = re.compile(r'\bq_\d+_vec\b')
        sql = vector_field_pattern.sub('embedding', sql)
        
        # å¤„ç†å­—æ®µåæ˜ å°„ - å°†ä¸å­˜åœ¨çš„å­—æ®µæ˜ å°„åˆ°å®é™…å­—æ®µ
        field_mappings = [
            (r'\bcontent\b(?!_)', 'content_with_weight'),  # content -> content_with_weight
            (r'\btitle\b(?!_)', 'title_tks'),  # title -> title_tks
        ]
        
        original_sql = sql
        for pattern, replacement in field_mappings:
            sql = re.sub(pattern, replacement, sql)
        
        # è®°å½•å­—æ®µæ˜ å°„æƒ…å†µï¼ˆä»…åœ¨å‘ç”Ÿæ˜ å°„æ—¶ï¼‰
        if sql != original_sql:
            logger.debug(f"Applied field mapping in SQL: {original_sql} -> {sql}")
        
        # å¤„ç†å…¨æ–‡æœç´¢ä¼˜åŒ– - æ·»åŠ ä¸ESç›¸åŒçš„å¤šè¯­è¨€åˆ†è¯å¤„ç†
        # å°†LIKEè½¬æ¢ä¸ºParadeDBçš„å…¨æ–‡æœç´¢è¯­æ³•ï¼Œå¹¶ä½¿ç”¨rag_tokenizerè¿›è¡Œåˆ†è¯
        replaces = []
        for r in re.finditer(r" ([a-z_]+_l?tks|content_with_weight)( like | ?= ?)'([^']+)'", sql):
            fld, v = r.group(1), r.group(3)
            # ä½¿ç”¨ä¸ESç›¸åŒçš„åˆ†è¯å¤„ç†é€»è¾‘
            tokenized_text = rag_tokenizer.fine_grained_tokenize(rag_tokenizer.tokenize(v))
            # ä½¿ç”¨ParadeDBçš„BM25å…¨æ–‡æœç´¢ï¼Œå¹¶å¤„ç†åˆ†è¯åçš„æ–‡æœ¬
            match = " {} @@@ '{}' ".format(fld, tokenized_text.replace('%', '').strip())
            replaces.append(
                ("{}{}'{}'".format(
                    r.group(1),
                    r.group(2),
                    r.group(3)),
                 match))

        for p, r in replaces:
            sql = sql.replace(p, r, 1)
            
        logger.debug(f"PDConnection.sql to paradedb: {sql}")
        
        # æ·»åŠ é‡è¯•æœºåˆ¶
        for i in range(ATTEMPT_TIME):
            conn = None
            try:
                conn = self._get_connection()
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    cur.execute(sql)
                    
                    if format == "json":
                        # è·å–æ‰€æœ‰åˆ—å
                        columns = [{"name": desc[0], "type": "text"} for desc in cur.description]
                        
                        # è·å–ç»“æœè¡Œ
                        rows = cur.fetchmany(fetch_size)
                        
                        # æ„å»ºä¸ESå…¼å®¹çš„å“åº”æ ¼å¼
                        result = {
                            "columns": columns,
                            "rows": [[row[col["name"]] for col in columns] for row in rows],
                            "cursor": None  # ParadeDBä¸æ”¯æŒæ¸¸æ ‡ï¼Œè®¾ä¸ºNone
                        }
                        
                        return result
                    else:
                        # è¿”å›é»˜è®¤æ ¼å¼
                        return {"results": [dict(row) for row in cur.fetchmany(fetch_size)]}
            except psycopg2.OperationalError as e:
                # è¿æ¥è¶…æ—¶å¤„ç†
                logger.warning(f"PDConnection.sql connection timeout: {str(e)}")
                if i < ATTEMPT_TIME - 1:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    time.sleep(3)  # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
                    continue
                return {"error": f"Connection error: {str(e)}"}
            except Exception as e:
                logger.exception(f"PDConnection.sql error: {str(e)}")
                # æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯
                error_msg = str(e)
                if "syntax error" in error_msg.lower():
                    error_msg = f"SQLè¯­æ³•é”™è¯¯: {error_msg}"
                elif "does not exist" in error_msg.lower():
                    error_msg = f"è¡¨æˆ–å­—æ®µä¸å­˜åœ¨: {error_msg}"
                return {"error": error_msg}
            finally:
                if conn:
                    self._return_connection(conn)
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        logger.error("PDConnection.sql timeout for all attempts!")
        return {"error": "æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"}

    def __del__(self):
        if hasattr(self, 'pool'):
            self.pool.closeall()

    def _upgrade_table_schema(self, indexName: str) -> bool:
        """
        å‡çº§ç°æœ‰è¡¨ç»“æ„ï¼Œæ·»åŠ ç¼ºå¤±çš„å­—æ®µ
        """
        conn = None
        try:
            conn = self._get_connection()
            with conn.cursor() as cur:
                # è·å–å½“å‰è¡¨çš„å­—æ®µåˆ—è¡¨å’Œç±»å‹
                cur.execute(f"""
                    SELECT column_name, data_type 
                    FROM information_schema.columns 
                    WHERE table_name = %s
                    ORDER BY column_name
                """, (indexName,))
                existing_columns_with_types = cur.fetchall()
                existing_columns = {row[0] for row in existing_columns_with_types}
                
                # æ‰“å°å½“å‰è¡¨ç»“æ„ç”¨äºè°ƒè¯•
                logger.info(f"Current table {indexName} schema:")
                for col_name, col_type in existing_columns_with_types:
                    logger.info(f"  {col_name}: {col_type}")
                
                # å®šä¹‰æ‰€æœ‰åº”è¯¥å­˜åœ¨çš„å­—æ®µåŠå…¶ç±»å‹
                required_fields = {
                    'content_sm_ltks': 'TEXT',
                    'title_sm_tks': 'TEXT',
                    'name_kwd': 'TEXT',
                    'important_tks': 'TEXT',
                    'question_tks': 'TEXT',
                    'create_time': 'TEXT',
                    'authors_tks': 'TEXT',
                    'authors_sm_tks': 'TEXT',
                    'weight_int': 'JSONB DEFAULT \'0\'',
                    'weight_flt': 'JSONB DEFAULT \'0.0\'',
                    'rank_int': 'JSONB DEFAULT \'0\'',
                    'rank_flt': 'JSONB DEFAULT \'0.0\'',
                    'knowledge_graph_kwd': 'TEXT',
                    'entities_kwd': 'TEXT',
                    'pagerank_fea': 'INTEGER DEFAULT 0',
                    'tag_feas': 'TEXT',
                    'tag_kwd': 'TEXT',
                    'from_entity_kwd': 'TEXT',
                    'to_entity_kwd': 'TEXT',
                    'entity_kwd': 'TEXT',
                    'entity_type_kwd': 'TEXT',
                    'source_id': 'TEXT',
                    'n_hop_with_weight': 'TEXT',
                    'removed_kwd': 'TEXT'
                }
                
                # æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
                added_fields = []
                modified_fields = []
                
                # éœ€è¦ä¿®æ”¹ç±»å‹çš„å­—æ®µæ˜ å°„ï¼ˆä»æ—§ç±»å‹åˆ°æ–°ç±»å‹ï¼‰
                type_changes = {
                    'page_num_int': ('integer', 'JSONB'),
                    'top_int': ('integer', 'JSONB'), 
                    'position_int': ('integer', 'JSONB'),
                    'weight_int': ('integer', 'JSONB'),
                    'weight_flt': ('double precision', 'JSONB'),
                    'rank_int': ('integer', 'JSONB'),
                    'rank_flt': ('double precision', 'JSONB')
                }
                
                # æ£€æŸ¥éœ€è¦ä¿®æ”¹ç±»å‹çš„å­—æ®µ
                for field_name, (old_type, new_type) in type_changes.items():
                    if field_name in existing_columns:
                        # æ£€æŸ¥å½“å‰å­—æ®µç±»å‹
                        cur.execute(f"""
                            SELECT data_type 
                            FROM information_schema.columns 
                            WHERE table_name = %s AND column_name = %s
                        """, (indexName, field_name))
                        current_type_result = cur.fetchone()
                        if current_type_result and current_type_result[0] == old_type:
                            try:
                                # ä¿®æ”¹å­—æ®µç±»å‹ - ä½¿ç”¨æ›´å®‰å…¨çš„è½¬æ¢æ–¹å¼
                                if new_type == 'JSONB':
                                    # å¯¹äºè½¬æ¢åˆ°JSONBçš„æƒ…å†µï¼Œå…ˆå°†æ•°æ®è½¬æ¢ä¸ºJSONæ ¼å¼
                                    cur.execute(f"""
                                        ALTER TABLE {indexName} 
                                        ALTER COLUMN {field_name} TYPE {new_type} 
                                        USING CASE 
                                            WHEN {field_name} IS NULL THEN NULL 
                                            ELSE to_jsonb({field_name}) 
                                        END
                                    """)
                                else:
                                    cur.execute(f"""
                                        ALTER TABLE {indexName} 
                                        ALTER COLUMN {field_name} TYPE {new_type} 
                                        USING {field_name}::text::{new_type}
                                    """)
                                modified_fields.append(f"{field_name}({old_type}->{new_type})")
                                logger.info(f"Modified field {field_name} type from {old_type} to {new_type} in table {indexName}")
                            except Exception as e:
                                logger.warning(f"Failed to modify field {field_name} type in table {indexName}: {str(e)}")
                                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•åˆ é™¤å¹¶é‡æ–°æ·»åŠ å­—æ®µ
                                try:
                                    logger.info(f"Attempting to drop and recreate field {field_name}")
                                    cur.execute(f"ALTER TABLE {indexName} DROP COLUMN IF EXISTS {field_name}")
                                    cur.execute(f"ALTER TABLE {indexName} ADD COLUMN {field_name} {new_type}")
                                    modified_fields.append(f"{field_name}(dropped+recreated)")
                                    logger.info(f"Successfully recreated field {field_name} as {new_type}")
                                except Exception as e2:
                                    logger.error(f"Failed to recreate field {field_name}: {str(e2)}")
                
                for field_name, field_type in required_fields.items():
                    if field_name not in existing_columns:
                        try:
                            cur.execute(f"ALTER TABLE {indexName} ADD COLUMN {field_name} {field_type}")
                            added_fields.append(field_name)
                            logger.info(f"Added missing field {field_name} to table {indexName}")
                        except Exception as e:
                            logger.warning(f"Failed to add field {field_name} to table {indexName}: {str(e)}")
                
                if added_fields or modified_fields:
                    conn.commit()
                    if added_fields:
                        logger.info(f"Successfully upgraded table {indexName}, added fields: {added_fields}")
                    if modified_fields:
                        logger.info(f"Successfully upgraded table {indexName}, modified fields: {modified_fields}")
                    return True
                else:
                    logger.info(f"Table {indexName} schema is up to date")
                    return True
                    
        except Exception as e:
            logger.exception(f"Failed to upgrade table {indexName} schema: {str(e)}")
            return False
        finally:
            if conn:
                self._return_connection(conn)

class ManagedConnection:
    """åŒ…è£…PostgreSQLè¿æ¥ï¼Œå®ç°ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥å£"""
    
    def __init__(self, connection_manager, conn):
        self.connection_manager = connection_manager
        self.conn = conn
        self._returned = False
        
    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        if not self._returned:
            self.connection_manager._return_connection(self.conn)
            self._returned = True
        
    def cursor(self, *args, **kwargs):
        return self.conn.cursor(*args, **kwargs)
        
    def commit(self):
        return self.conn.commit()
        
    def rollback(self):
        return self.conn.rollback()
        
    def __del__(self):
        if not self._returned:
            try:
                self.connection_manager._return_connection(self.conn)
                self._returned = True
            except:
                pass  # å¿½ç•¥ææ„æ—¶çš„é”™è¯¯